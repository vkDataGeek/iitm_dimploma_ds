{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 61246,
          "databundleVersionId": 6604167,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30558,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "22ds1000184-notebook-t32023",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkDataGeek/iitm_dimploma_ds/blob/main/22ds1000184_notebook_t32023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "tArV2nz2dwYm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "taxi_fare_guru_total_amount_prediction_challenge_path = kagglehub.competition_download('taxi-fare-guru-total-amount-prediction-challenge')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "dl0pmPtddwYp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Description"
      ],
      "metadata": {
        "id": "DTiZuYHSdwYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Taxi Fare Guru: Total Amount Prediction Challenge\"**\n",
        "- We need to build the most accurate models for predicting the total amount paid by travelers for taxi rides."
      ],
      "metadata": {
        "id": "ITpAR6YgdwYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:41.940093Z",
          "iopub.execute_input": "2023-11-23T16:44:41.940888Z",
          "iopub.status.idle": "2023-11-23T16:44:41.951655Z",
          "shell.execute_reply.started": "2023-11-23T16:44:41.940849Z",
          "shell.execute_reply": "2023-11-23T16:44:41.950395Z"
        },
        "trusted": true,
        "id": "j97-qjHMdwYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Package Imports"
      ],
      "metadata": {
        "id": "0c6bbwbodwYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import inspect\n",
        "from tabulate import tabulate\n",
        "\n",
        "#To plot pretty pictures\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mlt\n",
        "\n",
        "#Global Matplot setting\n",
        "mlt.rc('figure',figsize=(8,6))\n",
        "mlt.rc('axes',labelsize=6)\n",
        "mlt.rc('xtick',labelsize=6)\n",
        "mlt.rc('ytick',labelsize=6)\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "random_state=10\n",
        "np.random.seed(random_state)\n",
        "\n",
        "#ignore warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:41.984363Z",
          "iopub.execute_input": "2023-11-23T16:44:41.985577Z",
          "iopub.status.idle": "2023-11-23T16:44:42.000498Z",
          "shell.execute_reply.started": "2023-11-23T16:44:41.985537Z",
          "shell.execute_reply": "2023-11-23T16:44:41.998937Z"
        },
        "trusted": true,
        "id": "Gw4mtRwWdwYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "woZE2xVjdwYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Sample Data\n",
        "sample_data=pd.read_csv('/kaggle/input/taxi-fare-guru-total-amount-prediction-challenge/sample.csv.csv')\n",
        "orig_train_data= pd.read_csv('/kaggle/input/taxi-fare-guru-total-amount-prediction-challenge/train.csv')\n",
        "orig_test_data=  pd.read_csv('/kaggle/input/taxi-fare-guru-total-amount-prediction-challenge/test.csv')\n",
        "print('orig_test_data :', orig_test_data.shape)\n",
        "print('sample_data :', sample_data.shape)\n",
        "print('orig_train_data :',  orig_train_data.shape)\n",
        "\n",
        "# Always maintain original copy of data as best practices & further exploration on copy of it\n",
        "test_data = orig_test_data.copy()\n",
        "train_data = orig_train_data.copy()\n",
        "print(\"\")\n",
        "print('train_data :', train_data.shape)\n",
        "print('test_data :',  test_data.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:42.046387Z",
          "iopub.execute_input": "2023-11-23T16:44:42.046876Z",
          "iopub.status.idle": "2023-11-23T16:44:42.975183Z",
          "shell.execute_reply.started": "2023-11-23T16:44:42.046841Z",
          "shell.execute_reply": "2023-11-23T16:44:42.973743Z"
        },
        "trusted": true,
        "id": "LZSS1kCwdwYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(test_data.head(10))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:42.977731Z",
          "iopub.execute_input": "2023-11-23T16:44:42.97879Z",
          "iopub.status.idle": "2023-11-23T16:44:43.012624Z",
          "shell.execute_reply.started": "2023-11-23T16:44:42.978748Z",
          "shell.execute_reply": "2023-11-23T16:44:43.010814Z"
        },
        "trusted": true,
        "id": "96Yr6B5idwYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference:\n",
        "- Not needed to add ID column here, can be done directly during submission"
      ],
      "metadata": {
        "id": "BVAY2xnhdwYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. DummyClassifier"
      ],
      "metadata": {
        "id": "BWJ0WRsRdwYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Separate Train-Test data for Dummy Classifier\n",
        "- This is just for first(Draft) submission"
      ],
      "metadata": {
        "id": "E1nMDBTqdwYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DummyClassifier():\n",
        "    all_columns = train_data.columns\n",
        "    print(\"all_columns -> \", all_columns)\n",
        "    # Copy all features leaving aside the label.\n",
        "    X_dummy = train_data.drop(\"total_amount\", axis=1)\n",
        "    # Copy the label list\n",
        "    y_dummy = train_data['total_amount']\n",
        "    print('X_dummy :', X_dummy.shape)\n",
        "    print('y_dummy :',  y_dummy.shape)\n",
        "\n",
        "    # Splitting X & Y for train/test: Validation Copy from Train Data\n",
        "    X_train_dummy,X_test_dummy,y_train_dummy,y_test_dummy = train_test_split(X_dummy,y_dummy,random_state=random_state,test_size=0.1)\n",
        "    print('Training dataset size -> ', X_train_dummy.shape, ' & ', y_train_dummy.shape)\n",
        "    print('Validation dataset size -> ', X_test_dummy.shape, ' & ', y_test_dummy.shape)\n",
        "    return DummyClassifier_First_Submission.__call__\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:43.014462Z",
          "iopub.execute_input": "2023-11-23T16:44:43.015171Z",
          "iopub.status.idle": "2023-11-23T16:44:43.02523Z",
          "shell.execute_reply.started": "2023-11-23T16:44:43.0151Z",
          "shell.execute_reply": "2023-11-23T16:44:43.02368Z"
        },
        "trusted": true,
        "id": "dj42aVyDdwYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DummyClassifier (First Submission)"
      ],
      "metadata": {
        "id": "zggQOLz-dwYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DummyClassifier_First_Submission():\n",
        "    from sklearn.dummy import DummyRegressor\n",
        "    dummy_regr = DummyRegressor(strategy=\"mean\")\n",
        "    dummy_regr.fit(X_train_dummy, y_train_dummy)\n",
        "    y_test_pred = dummy_regr.predict(X_test_dummy)\n",
        "    X_test_r2Score= dummy_regr.score(X_test_dummy, y_test_dummy)\n",
        "    print(\"X_test_r2Score: \", X_test_r2Score)\n",
        "\n",
        "    test_data_dummy = test_data.copy()\n",
        "    test_data_dummy_pred = dummy_regr.predict(test_data_dummy)\n",
        "    test_data_dummy_pred.shape\n",
        "    return test_data_dummy_pred\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:43.028407Z",
          "iopub.execute_input": "2023-11-23T16:44:43.028854Z",
          "iopub.status.idle": "2023-11-23T16:44:43.044915Z",
          "shell.execute_reply.started": "2023-11-23T16:44:43.02881Z",
          "shell.execute_reply": "2023-11-23T16:44:43.04395Z"
        },
        "trusted": true,
        "id": "nxMLNkp4dwYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "HFX6vyMNdwYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Statistics and data info**"
      ],
      "metadata": {
        "id": "AxchACoodwYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_data.info())\n",
        "\n",
        "\"\"\" Columns with Null:\n",
        "passenger_count        168923\n",
        "RatecodeID             168923\n",
        "store_and_fwd_flag     168923\n",
        "congestion_surcharge   168923\n",
        "Airport_fee            168923\n",
        "\"\"\"\n",
        "\n",
        "print(\"\")\n",
        "Num_features = train_data.select_dtypes(['int', 'float']).columns\n",
        "Cat_features = train_data.select_dtypes(['object']).columns\n",
        "print(\"Num_features -> \", Num_features)\n",
        "print(\"Cat_features -> \", Cat_features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:43.046796Z",
          "iopub.execute_input": "2023-11-23T16:44:43.047593Z",
          "iopub.status.idle": "2023-11-23T16:44:43.20179Z",
          "shell.execute_reply.started": "2023-11-23T16:44:43.047535Z",
          "shell.execute_reply": "2023-11-23T16:44:43.200768Z"
        },
        "trusted": true,
        "id": "kqoXvsNsdwYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head(6)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:43.20322Z",
          "iopub.execute_input": "2023-11-23T16:44:43.203776Z",
          "iopub.status.idle": "2023-11-23T16:44:43.232394Z",
          "shell.execute_reply.started": "2023-11-23T16:44:43.203745Z",
          "shell.execute_reply": "2023-11-23T16:44:43.230525Z"
        },
        "trusted": true,
        "id": "WA0mQ-RvdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_data['total_amount'].describe())\n",
        "train_data.describe(include = 'all')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:43.23446Z",
          "iopub.execute_input": "2023-11-23T16:44:43.234863Z",
          "iopub.status.idle": "2023-11-23T16:44:43.812793Z",
          "shell.execute_reply.started": "2023-11-23T16:44:43.234811Z",
          "shell.execute_reply": "2023-11-23T16:44:43.81137Z"
        },
        "trusted": true,
        "id": "gpI5aZFwdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Missing values - Null & Unknowns**"
      ],
      "metadata": {
        "id": "HGqdVt0LdwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Train Data --\\n\")\n",
        "print(train_data.nunique())\n",
        "\n",
        "print(\"\\n --- Test Data --\\n\")\n",
        "print(test_data.nunique())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:43.81458Z",
          "iopub.execute_input": "2023-11-23T16:44:43.815072Z",
          "iopub.status.idle": "2023-11-23T16:44:44.05457Z",
          "shell.execute_reply.started": "2023-11-23T16:44:43.815027Z",
          "shell.execute_reply": "2023-11-23T16:44:44.053209Z"
        },
        "trusted": true,
        "id": "2OFrReUvdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- isnull & Unknowns in Data --\")\n",
        "train_data.isin([\"?\",\"unknown\",\"N/A\",\"n/a\", np.nan]).sum() # checking unknown values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:44.055893Z",
          "iopub.execute_input": "2023-11-23T16:44:44.056273Z",
          "iopub.status.idle": "2023-11-23T16:44:44.762189Z",
          "shell.execute_reply.started": "2023-11-23T16:44:44.056243Z",
          "shell.execute_reply": "2023-11-23T16:44:44.760808Z"
        },
        "trusted": true,
        "id": "VkmQDQkXdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.isin([\"?\",\"unknown\",\"N/A\",\"n/a\", np.nan]).sum() # checking unknown values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:44.766609Z",
          "iopub.execute_input": "2023-11-23T16:44:44.767038Z",
          "iopub.status.idle": "2023-11-23T16:44:44.916096Z",
          "shell.execute_reply.started": "2023-11-23T16:44:44.766993Z",
          "shell.execute_reply": "2023-11-23T16:44:44.914683Z"
        },
        "trusted": true,
        "id": "0fI0WLsQdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Analysing Target Label: Total_amount**"
      ],
      "metadata": {
        "id": "o7xxCJtQdwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distrubution of target variable with skewness\n",
        "fig, ax = plt.subplots(figsize = (8,5))\n",
        "sns.distplot(train_data.total_amount, bins = 200, color = 'firebrick', ax = ax)\n",
        "ax.set_title('Distribution of total_amount having skewness: {:0.5}'.format(train_data.total_amount.skew()))\n",
        "ax.set_ylabel(' frequency')\n",
        "plt.show()\n",
        "\n",
        "print(\"** Train **\\n\")\n",
        "print(f\"Records with {len(train_data[train_data['total_amount'] < -20])} (<0)negative fares.\")\n",
        "print(f\"Records with {len(train_data[train_data['total_amount'] == 0])} #0 fares.\")\n",
        "print(f\"Records with {len(train_data[train_data['total_amount'] > 130])} fares greater than #130.\")\n",
        "print(\"Train min: \", np.amin(train_data.total_amount),\", max: \", np.amax(train_data.total_amount))\n",
        "print(train_data.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:44.917487Z",
          "iopub.execute_input": "2023-11-23T16:44:44.917839Z",
          "iopub.status.idle": "2023-11-23T16:44:46.398699Z",
          "shell.execute_reply.started": "2023-11-23T16:44:44.917811Z",
          "shell.execute_reply": "2023-11-23T16:44:46.397425Z"
        },
        "trusted": true,
        "id": "eHugtNrNdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Data seems right skewed and we will remove outliers in later section.\n",
        "- We can consider to keep Normal distribution range (95%) of data between 2.5 and 97.5 quantile range. And, only 4% (~8k) data are outside 95% range - we can drop them.\n",
        "> We can even consider larger range of data between 0.05 and 99.5 quantile to include more records (as test data also has value in similar range)."
      ],
      "metadata": {
        "id": "jT6MNkOqdwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting Quantile values:\n",
        "low_quantile = train_data.total_amount.quantile(0.005)\n",
        "high_quantile = train_data.total_amount.quantile(0.995)\n",
        "print(low_quantile, high_quantile)\n",
        "print(\"Count < 0: \", train_data[train_data['total_amount'] <0].total_amount.count())\n",
        "quantile_rng_count=train_data[(train_data['total_amount'] < low_quantile) | (train_data['total_amount'] > high_quantile) ].total_amount.count()\n",
        "print(\"Count outside Quantile range: \", quantile_rng_count, \" , % count: \", quantile_rng_count *100/len(train_data) )\n",
        "\n",
        "## 0.025 - 0.975: 8457 - 4.83%\n",
        "## 0.010 - 0.995: 2623 - 1.49%\n",
        "## 0.015 - 0.995: 3466 - 1.98%"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:46.400257Z",
          "iopub.execute_input": "2023-11-23T16:44:46.400607Z",
          "iopub.status.idle": "2023-11-23T16:44:46.424141Z",
          "shell.execute_reply.started": "2023-11-23T16:44:46.400578Z",
          "shell.execute_reply": "2023-11-23T16:44:46.422755Z"
        },
        "trusted": true,
        "id": "MATEtgdndwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Reviewing Passenger_Count**"
      ],
      "metadata": {
        "id": "QwWM7qpOdwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distrubution of passenger_count\n",
        "fig, ax = plt.subplots(figsize = (6,4))\n",
        "class_dist = train_data.passenger_count.value_counts()\n",
        "class_dist.plot(kind = 'bar', ax = ax)\n",
        "ax.set_title('Class distribution of passenger_count')\n",
        "ax.set_ylabel('absolute frequency')\n",
        "plt.show()\n",
        "\n",
        "display(class_dist)\n",
        "print(\"\\n-- test data --\")\n",
        "class_dist_test = test_data.passenger_count.value_counts()\n",
        "display(class_dist_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:46.425641Z",
          "iopub.execute_input": "2023-11-23T16:44:46.426119Z",
          "iopub.status.idle": "2023-11-23T16:44:46.722726Z",
          "shell.execute_reply.started": "2023-11-23T16:44:46.426059Z",
          "shell.execute_reply": "2023-11-23T16:44:46.721397Z"
        },
        "trusted": true,
        "id": "NV7ls0PTdwYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The class distribution is imbalanced since some classes outnumber some other classes in TRAIN data. Clearly, With passenger Count #1 is most frequent and anything beyond\n",
        " 6 seems unreasonable.\n",
        "- Also reviewing same for TEST data, count > 6 is only 1.\n",
        "\n",
        "> Inference\n",
        "\n",
        "- so we can drop records with count 8 & 9 OR SET Max number of Passenger to 6."
      ],
      "metadata": {
        "id": "GkufSdlWdwYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Reviewing Vendor_ID**"
      ],
      "metadata": {
        "id": "2913Ti78dwYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_data.VendorID.value_counts())\n",
        "\n",
        "# create grouped boxplot\n",
        "sns.boxplot(x = train_data.VendorID,\n",
        "            y = train_data.total_amount,\n",
        "            hue = train_data.RatecodeID)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:46.724363Z",
          "iopub.execute_input": "2023-11-23T16:44:46.724744Z",
          "iopub.status.idle": "2023-11-23T16:44:47.367813Z",
          "shell.execute_reply.started": "2023-11-23T16:44:46.724715Z",
          "shell.execute_reply": "2023-11-23T16:44:47.366624Z"
        },
        "trusted": true,
        "id": "n7XLUQ0ddwYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most number of records is with Vedor_id =1 and even negative Fare price is also with same vendor (#1) - having rateCode_id #4 & #5"
      ],
      "metadata": {
        "id": "m0i8ZUEjdwYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Analysing Drop & Pickup timing**"
      ],
      "metadata": {
        "id": "3nzK3GFJdwYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_del = pd.to_datetime(train_data['tpep_dropoff_datetime']) - pd.to_datetime(train_data['tpep_pickup_datetime'])\n",
        "df_r = train_data.copy()\n",
        "df_r['ride_duration'] = time_del.dt.total_seconds()/60.0 #value in minutes\n",
        "df_r.ride_duration.value_counts().sort_values(ascending=True, na_position='first')\n",
        "print(\"% of records with -ve value:\", len(df_r[df_r['ride_duration'] < 0])*100/len(df_r), \"%\")\n",
        "\n",
        "print(\"\\nride_duration -> Describe()\")\n",
        "\n",
        "display(df_r['ride_duration'].describe())\n",
        "print(f\"Records with {len(df_r[df_r['ride_duration'] < 0])} (<0)negative ride_duration.\")\n",
        "print(f\"Records with {len(df_r[df_r['ride_duration'] == 0])} #0 ride_duration.\")\n",
        "print(f\"Records with {len(df_r[df_r['ride_duration'] > 300])} ride_duration greater than #300.\")\n",
        "display(df_r[df_r.ride_duration > 300].head(3))\n",
        "\n",
        "#display(df_r[df_r['ride_duration'] < 0])\n",
        "print(\"\\n\")\n",
        "\n",
        "display(\"--- Reviewing Pickup-Drop time in Test Data as well ---\")\n",
        "time_del_test = pd.to_datetime(test_data['tpep_dropoff_datetime']) - pd.to_datetime(test_data['tpep_pickup_datetime'])\n",
        "df_r_test = test_data.copy()\n",
        "df_r_test['ride_duration'] = time_del_test.dt.total_seconds()/60.0 #value in minutes\n",
        "print(\"% of records with -ve value (Test data):\", len(df_r_test[df_r_test['ride_duration'] < 0])*100/len(df_r_test))\n",
        "#display(df_r_test[df_r_test['ride_duration'] < 0])\n",
        "display(df_r_test['ride_duration'].describe())\n",
        "print(f\"Records with {len(df_r_test[df_r_test['ride_duration'] < 0])} (<0)negative ride_duration.\")\n",
        "print(f\"Records with {len(df_r_test[df_r_test['ride_duration'] == 0])} #0 ride_duration.\")\n",
        "print(f\"Records with {len(df_r_test[np.absolute(df_r_test.ride_duration > 300)])} ride_duration greater than #300.\")\n",
        "display(df_r_test[np.absolute(df_r_test.ride_duration > 300)].head(3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.369492Z",
          "iopub.execute_input": "2023-11-23T16:44:47.370886Z",
          "iopub.status.idle": "2023-11-23T16:44:47.709483Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.370839Z",
          "shell.execute_reply": "2023-11-23T16:44:47.708095Z"
        },
        "trusted": true,
        "id": "RllBJ5ysdwYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> - Some case trip duration(in Minutes) are negative values. And count of such records are significant (37+ %).\n",
        "> - Also, there seems to be some outlier for Trip_Duration with values > 600 mins. This issue is prevalent in Test data as well.\n",
        "\n",
        "- Inference:\n",
        "> **We can derive ride Time & PickUp Hour - which can potentially impact total Fare**\n",
        "> **We can drop DateTime timestamp from model analysis. However, Hour of booking might impact Total price and hence keeping Hour values from Pickup time.**"
      ],
      "metadata": {
        "id": "h8XmsgALdwYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train_data.PULocationID.value_counts().sort_values(ascending=False, na_position='first')\n",
        "train_data.DOLocationID.value_counts().sort_values(ascending=False, na_position='first')\n",
        "\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.711409Z",
          "iopub.execute_input": "2023-11-23T16:44:47.711879Z",
          "iopub.status.idle": "2023-11-23T16:44:47.723641Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.711835Z",
          "shell.execute_reply": "2023-11-23T16:44:47.722298Z"
        },
        "trusted": true,
        "id": "0n_QMBd8dwYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Inference: PULocationID & DOLocationID can be dropped as 260+ unique IDs are present. And, count of records per LocationID is almost evenly distributed."
      ],
      "metadata": {
        "id": "lSPpZY0SdwY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Identity Columns**"
      ],
      "metadata": {
        "id": "mKqg82iAdwY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can drop below ID type columns which has least significance on resultset: store_and_fwd_flag,PULocationID, DOLocationID**"
      ],
      "metadata": {
        "id": "SWADCIYbdwY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_cols = [\"store_and_fwd_flag\",\"PULocationID\",\"DOLocationID\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.725092Z",
          "iopub.execute_input": "2023-11-23T16:44:47.725547Z",
          "iopub.status.idle": "2023-11-23T16:44:47.740129Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.725512Z",
          "shell.execute_reply": "2023-11-23T16:44:47.739185Z"
        },
        "trusted": true,
        "id": "bXiEAbpVdwY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Different Surcharge Features**"
      ],
      "metadata": {
        "id": "RrtVBVyKdwY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "improvement_surcharge        4\n",
        "congestion_surcharge         3\n",
        "Airport_fee                  3\n",
        "\"\"\"\n",
        "display(train_data.improvement_surcharge.value_counts().sort_values(ascending=False, na_position='first'))\n",
        "print(\"---\")\n",
        "display(train_data.congestion_surcharge.value_counts().sort_values(ascending=False, na_position='first'))\n",
        "print(\"---\")\n",
        "display(train_data.Airport_fee.value_counts().sort_values(ascending=False, na_position='first'))\n",
        "print(\"---\")\n",
        "display(train_data.payment_type.value_counts().sort_values(ascending=False, na_position='first'))\n",
        "print(\"---\")\n",
        "display(train_data.RatecodeID.value_counts().sort_values(ascending=False, na_position='first'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.741277Z",
          "iopub.execute_input": "2023-11-23T16:44:47.741665Z",
          "iopub.status.idle": "2023-11-23T16:44:47.823649Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.741634Z",
          "shell.execute_reply": "2023-11-23T16:44:47.822424Z"
        },
        "trusted": true,
        "id": "gUVjEVUAdwY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Imputer to fill Values:\n",
        "* \"passenger_count\" - Median\n",
        "* \"congestion_surcharge\" - Between median or constant (0.0)    #assuming no additional charge\n",
        "* \"Airport_fee\" - Between median or constant (0.0)            #assuming no additional charge\n",
        "* payment_type (Unknown) - Median   #Setting value to NULL to use imputer"
      ],
      "metadata": {
        "id": "fZFRK4WOdwY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Trip Distance vs Trip Amount Analysis**"
      ],
      "metadata": {
        "id": "NcPz6JA-dwY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"trip_distance -> \", np.amin(train_data['trip_distance']), np.amax(train_data['trip_distance']))\n",
        "display((train_data['trip_distance'] > 200).value_counts())\n",
        "print(\"\\n --- Train --\")\n",
        "print(f\"Records with {len(train_data[train_data['trip_distance'] < 0])} (<0)negative trip_distance.\")\n",
        "print(f\"Records with {len(train_data[train_data['trip_distance'] == 0])} #0 trip_distance.\")\n",
        "print(f\"Records with {len(train_data[train_data['trip_distance'] > 200])} trip_distance greater than #200.\")\n",
        "display(train_data[train_data.trip_distance > 200])\n",
        "\n",
        "print(\"\\n --- Test --\")\n",
        "print(f\"Records with {len(test_data[test_data['trip_distance'] < 0])} (<0)negative trip_distance.\")\n",
        "print(f\"Records with {len(test_data[test_data['trip_distance'] == 0])} #0 trip_distance.\")\n",
        "print(f\"Records with {len(test_data[test_data['trip_distance'] > 200])} trip_distance greater than #200.\")\n",
        "display(test_data[test_data.trip_distance > 200])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.825728Z",
          "iopub.execute_input": "2023-11-23T16:44:47.826252Z",
          "iopub.status.idle": "2023-11-23T16:44:47.893542Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.826194Z",
          "shell.execute_reply": "2023-11-23T16:44:47.892118Z"
        },
        "trusted": true,
        "id": "FIAw8WaHdwY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Inference: Out of 175K records,\n",
        "trip_distance -> only 4 of them has values greater than 150 unit of distance in TRAIN data. We need to handle these Outlier records.\n",
        "Also, looking at TEST data, 2 records have trip_distance > 150. We can consider to set Max Trip distance as 150.\n",
        "\n",
        "> In both TRAIN & TEST dataset, 2632 & 742 respectively, has Trip distance as Zero but significant ride duration"
      ],
      "metadata": {
        "id": "6NTEeGZodwY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleansing modules"
      ],
      "metadata": {
        "id": "-Zwgn3pTdwY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Adding ID column to Train & Test Data\n",
        "\"\"\"\n",
        "def add_id_column_to_dataset(df):\n",
        "    print(\"Dataset Shape: \", df.shape)\n",
        "    df_id = pd.DataFrame({'ID': (i for i in np.arange(1,df.shape[0]+1)) })\n",
        "    frames=[df_id, df]\n",
        "    new_df = pd.concat(frames,axis=1)\n",
        "    print(\"New Dataset Shape: \", new_df.shape)\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Method to drop columns from DF\n",
        "\"\"\"\n",
        "def drop_columns_from_df(df2, colums_to_drop_list):\n",
        "    print(\"Columns to drop from DF: \", colums_to_drop_list)\n",
        "    new_df = df2.drop(colums_to_drop_list, axis=1, inplace = False)\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Update Payment_Type column 'unknown' values to Nan\n",
        "\"\"\"\n",
        "def update_payment_unknown_type(df):\n",
        "    #print(\"Before (update_payment_unknown_type): \", train_data.isnull().sum())\n",
        "    new_df = df.copy()\n",
        "    new_df['payment_type'] = df['payment_type'].replace(['unknown'], np.nan)\n",
        "    print(\"After (update_payment_unknown_type): \", train_data.isnull().sum())\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Filter Outlier Trip Distance rows from dataset. Instead of dropping the records, we can set Max value to 150\n",
        "\"\"\"\n",
        "def filter_outlier_trip_distance(df,allow_row_filter):\n",
        "    \"\"\"\n",
        "    if allow_row_filter:\n",
        "        #df['trip_distance'] = np.where(df['trip_distance']> 200, 200, df['trip_distance'])\n",
        "        new_df = df[(df['trip_distance'] <= 200)]\n",
        "    else:\n",
        "        new_df = df.copy()\n",
        "    \"\"\"\n",
        "    df['trip_distance'] = np.where(df['trip_distance']>= 200, 200, df['trip_distance'])\n",
        "    new_df = df.copy()\n",
        "    print(\"Max Trip Distance: \", np.amax(new_df.trip_distance))\n",
        "    return new_df\n",
        "\n",
        "\"\"\"\n",
        "Filter Outlier Passenger Count rows from dataset\n",
        "\"\"\"\n",
        "def filter_outlier_passenger_count(df):\n",
        "    #new_df = df[df.passenger_count <= 6]\n",
        "    #df['passenger_count'] = np.where(df['passenger_count']> 6, 6, df['passenger_count'])\n",
        "    print(\"Max passenger_count: \", np.amax(df.passenger_count))\n",
        "    return df\n",
        "\n",
        "\"\"\"\n",
        "Filter Outlier Total_amount - Using Quantile\n",
        "\"\"\"\n",
        "def filter_outlier_total_amount(df):\n",
        "    low_quantile = df.total_amount.quantile(0.001)   # 2.5 Quantile # 1.5 Q to drop less rec\n",
        "    high_quantile = df.total_amount.quantile(0.999)  # 97.5 Quantile\n",
        "    print(\"Quantile values for Total_amount: \", low_quantile, high_quantile)\n",
        "    new_df = df[(df['total_amount'] >= low_quantile) & (df['total_amount'] <= high_quantile)]\n",
        "    new_df['total_amount_dist'] = (np.floor(new_df.total_amount/5))\n",
        "    return new_df\n",
        "\n",
        "\"\"\"\n",
        "Derive ride Time & PickUp Hour - which can potentially impact total Fare\n",
        "Drop DateTime timestamp from model analysis.\n",
        "\"\"\"\n",
        "def data_prep_with_pickup_dropoff_time(df):\n",
        "    df['pickup_hour']=pd.to_datetime(df['tpep_pickup_datetime']).dt.hour\n",
        "    time_delta = pd.to_datetime(df['tpep_dropoff_datetime']) - pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "    #df['ride_duration'] = np.absolute(time_delta.dt.total_seconds()/60.0) #value in minutes  ## Absoulte value - negative duration doesn't makle sense\n",
        "    df['ride_duration'] = time_delta.dt.total_seconds()/60.0 #value in minutes\n",
        "    ts_cols = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"]\n",
        "    df = drop_columns_from_df(df, ts_cols)\n",
        "    print(\"New DF shape: \", df.shape)\n",
        "    return df\n",
        "\n",
        "def condition_based_df_update(df):\n",
        "    # Negative Ride duration doesn;t make sense - so adjusting the value\n",
        "    #df['ride_duration'] = np.where(df.ride_duration <= 0, np.nan , df['ride_duration'])\n",
        "    df['trip_distance'] = np.where(np.absolute(df['trip_distance'])> 600, 600, df['trip_distance'])\n",
        "    # Negative Tip/surcharge/additional Fee doesn't make sense :\n",
        "\n",
        "    #df['tip_amount'] = np.where(df.tip_amount < 0, np.nan, df['tip_amount'])\n",
        "    #df['tolls_amount'] = np.where(df.tolls_amount < 0, np.nan, df['tolls_amount'])\n",
        "    #df['improvement_surcharge'] = np.where(df.improvement_surcharge < 0, np.nan, df['improvement_surcharge'])\n",
        "    #df['congestion_surcharge'] = np.where(df.congestion_surcharge < 0, np.nan, df['congestion_surcharge'])\n",
        "    #df['Airport_fee'] = np.where(df.Airport_fee < 0, np.nan, df['Airport_fee'])\n",
        "    #df['extra'] = np.where(df.extra < 0, np.nan, df['extra'])\n",
        "\n",
        "    #df['Airport_fee'] = np.absolute(df.Airport_fee)\n",
        "    print(\"New DF shape (after condition_based_df_update): \", df.shape)\n",
        "    return df\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.895612Z",
          "iopub.execute_input": "2023-11-23T16:44:47.896257Z",
          "iopub.status.idle": "2023-11-23T16:44:47.916939Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.896208Z",
          "shell.execute_reply": "2023-11-23T16:44:47.915675Z"
        },
        "trusted": true,
        "id": "q5jWgKSFdwY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleansing_module(dataset_df, allow_row_filter):\n",
        "    print(\"Count(original) of given dataset: \", len(dataset_df))\n",
        "    dataset_df = add_id_column_to_dataset(dataset_df)\n",
        "    if allow_row_filter:\n",
        "        outlier_total_amount = filter_outlier_total_amount(dataset_df)\n",
        "        print(\"Count after outlier_total_amount(using Quantile): \", len(outlier_total_amount))\n",
        "\n",
        "        dataset_df=outlier_total_amount\n",
        "\n",
        "    df0 = filter_outlier_passenger_count(dataset_df)\n",
        "    df1 = filter_outlier_trip_distance(df0,allow_row_filter)\n",
        "    df_2 = update_payment_unknown_type(df1)\n",
        "    df_3 = data_prep_with_pickup_dropoff_time(df_2)\n",
        "    df_4 = condition_based_df_update(df_3)\n",
        "    cleansed_df = drop_columns_from_df(df_4, id_cols)\n",
        "    print(\"\\n** updated shape after all changes: \", cleansed_df.shape,\"\\n\")\n",
        "    print(cleansed_df.head(3))\n",
        "    return cleansed_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.918472Z",
          "iopub.execute_input": "2023-11-23T16:44:47.918894Z",
          "iopub.status.idle": "2023-11-23T16:44:47.9349Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.918805Z",
          "shell.execute_reply": "2023-11-23T16:44:47.933394Z"
        },
        "trusted": true,
        "id": "TBonAOxkdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **- Apply cleansing to Train Data -**"
      ],
      "metadata": {
        "id": "JV1ZH1pQdwY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleansed_train_dataset = cleansing_module(train_data, True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:47.936132Z",
          "iopub.execute_input": "2023-11-23T16:44:47.936513Z",
          "iopub.status.idle": "2023-11-23T16:44:49.372427Z",
          "shell.execute_reply.started": "2023-11-23T16:44:47.936479Z",
          "shell.execute_reply": "2023-11-23T16:44:49.371148Z"
        },
        "trusted": true,
        "id": "1RkUF2QUdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling UnderSampling Cases"
      ],
      "metadata": {
        "id": "HzQZcpbfdwY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_undersampling_cases(cleansed_data):\n",
        "    #df1 = cleansed_data[(np.absolute(cleansed_data.total_amount > 120)) | (cleansed_data.trip_distance >= 150) | (np.absolute(cleansed_data.ride_duration >= 500)) ]\n",
        "    df1 = cleansed_data[(np.absolute(cleansed_data.total_amount > 110)) ]\n",
        "    df2 = pd.concat([df1]*20, ignore_index=True)\n",
        "    display(df2.shape, cleansed_data.shape)\n",
        "    op_df = pd.concat([cleansed_data, df2], ignore_index=True)\n",
        "    display(op_df.shape)\n",
        "    return op_df\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:49.374431Z",
          "iopub.execute_input": "2023-11-23T16:44:49.374973Z",
          "iopub.status.idle": "2023-11-23T16:44:49.383726Z",
          "shell.execute_reply.started": "2023-11-23T16:44:49.374926Z",
          "shell.execute_reply": "2023-11-23T16:44:49.382116Z"
        },
        "trusted": true,
        "id": "WNCwC1KLdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleansed_train_data = add_undersampling_cases(cleansed_train_dataset)\n",
        "cleansed_train_data = cleansed_train_dataset.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:49.385077Z",
          "iopub.execute_input": "2023-11-23T16:44:49.38546Z",
          "iopub.status.idle": "2023-11-23T16:44:49.416387Z",
          "shell.execute_reply.started": "2023-11-23T16:44:49.38543Z",
          "shell.execute_reply": "2023-11-23T16:44:49.415186Z"
        },
        "trusted": true,
        "id": "jGc1kYbwdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distrubution of target variable with skewness\n",
        "fig, ax = plt.subplots(figsize = (8,5))\n",
        "sns.distplot(cleansed_train_data.total_amount, bins = 200, color = 'firebrick', ax = ax)\n",
        "ax.set_title('Distribution of total_amount having skewness: {:0.5}'.format(cleansed_train_data.total_amount.skew()))\n",
        "ax.set_ylabel(' frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:49.41792Z",
          "iopub.execute_input": "2023-11-23T16:44:49.418273Z",
          "iopub.status.idle": "2023-11-23T16:44:50.990594Z",
          "shell.execute_reply.started": "2023-11-23T16:44:49.418243Z",
          "shell.execute_reply": "2023-11-23T16:44:50.98919Z"
        },
        "trusted": true,
        "id": "L45271KudwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization Post Cleansing"
      ],
      "metadata": {
        "id": "WeNPTEGJdwY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((cleansed_train_data['total_amount'] > 0).value_counts())\n",
        "plt.rc('figure',figsize=(8,5))\n",
        "sns.histplot(cleansed_train_data['total_amount'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:50.992153Z",
          "iopub.execute_input": "2023-11-23T16:44:50.992554Z",
          "iopub.status.idle": "2023-11-23T16:44:52.475169Z",
          "shell.execute_reply.started": "2023-11-23T16:44:50.992517Z",
          "shell.execute_reply": "2023-11-23T16:44:52.473848Z"
        },
        "trusted": true,
        "id": "mtF6rUVSdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Total_amount < 0 doesn't make sense and can be dropped."
      ],
      "metadata": {
        "id": "cqzPtGbldwY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rc('figure',figsize=(8,5))\n",
        "sns.histplot(cleansed_train_data['total_amount_dist'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:52.47667Z",
          "iopub.execute_input": "2023-11-23T16:44:52.477055Z",
          "iopub.status.idle": "2023-11-23T16:44:54.015569Z",
          "shell.execute_reply.started": "2023-11-23T16:44:52.477021Z",
          "shell.execute_reply": "2023-11-23T16:44:54.013637Z"
        },
        "trusted": true,
        "id": "HHW8LGtDdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot y vs x\n",
        "plt.rc('figure',figsize=(8,5))\n",
        "plt.plot(cleansed_train_data['trip_distance'], cleansed_train_data['total_amount'])\n",
        "plt.title('Trip Distance vs Total Amount')\n",
        "plt.xlabel('trip_distance')\n",
        "plt.ylabel('total_amount')\n",
        "\n",
        "#show plot to user\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:54.025164Z",
          "iopub.execute_input": "2023-11-23T16:44:54.025616Z",
          "iopub.status.idle": "2023-11-23T16:44:56.574505Z",
          "shell.execute_reply.started": "2023-11-23T16:44:54.025572Z",
          "shell.execute_reply": "2023-11-23T16:44:56.573142Z"
        },
        "trusted": true,
        "id": "5kZAwGEYdwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Corrrelation of Features**"
      ],
      "metadata": {
        "id": "UOSvb3iddwY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = cleansed_train_data.drop(['payment_type'], axis=1).corr()\n",
        "#corr_no_us = cleansed_train_dataset.drop(['payment_type'], axis=1).corr()\n",
        "\n",
        "mlt.rc('figure',figsize=(10,8))\n",
        "sns.heatmap(corr, annot= True)\n",
        "print(\"\\n Target Label Corr:\\n\",  corr['total_amount'])\n",
        "#print(\"\\n Target Label corr_no_us :\\n\",  corr_no_us['total_amount'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:56.576273Z",
          "iopub.execute_input": "2023-11-23T16:44:56.576668Z",
          "iopub.status.idle": "2023-11-23T16:44:58.138995Z",
          "shell.execute_reply.started": "2023-11-23T16:44:56.576634Z",
          "shell.execute_reply": "2023-11-23T16:44:58.137746Z"
        },
        "trusted": true,
        "id": "dkpOm933dwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weak_corr_columns = corr[np.abs(corr['total_amount']) < 0.055 ]['total_amount'].index\n",
        "print(\"\\n Actual weak_corr_columns: \", weak_corr_columns)\n",
        "#weak_corr_columns = ['passenger_count', 'RatecodeID', 'improvement_surcharge', 'pickup_hour']\n",
        "weak_corr_columns = ['improvement_surcharge', 'pickup_hour']\n",
        "cleansed_train_data_wkCorr = drop_columns_from_df(cleansed_train_data, weak_corr_columns)\n",
        "#cleansed_train_data_new = drop_columns_from_df(cleansed_train_data_wkCorr, ['total_amount_dist'])\n",
        "\n",
        "print(\"\\n cleansed_train_data_wkCorr to Pre-Processor -> \", cleansed_train_data_wkCorr.shape)\n",
        "print(cleansed_train_data_wkCorr.columns)\n",
        "#display(\"\\n\",cleansed_train_data_new.head(2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:58.140527Z",
          "iopub.execute_input": "2023-11-23T16:44:58.140914Z",
          "iopub.status.idle": "2023-11-23T16:44:58.160601Z",
          "shell.execute_reply.started": "2023-11-23T16:44:58.140876Z",
          "shell.execute_reply": "2023-11-23T16:44:58.159045Z"
        },
        "trusted": true,
        "id": "eWdQAtBudwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleansed_train_data_plot = cleansed_train_data.copy()\n",
        "cleansed_train_data_plot['total_amount_dist10'] = np.floor((cleansed_train_data_plot.total_amount)/10)\n",
        "sns.pairplot(cleansed_train_data_plot,\n",
        "             hue=\"total_amount_dist10\",\n",
        "             x_vars=[\"total_amount\", \"trip_distance\", \"ride_duration\",\"Airport_fee\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\"],\n",
        "             y_vars=[\"total_amount\", \"trip_distance\", \"ride_duration\",\"Airport_fee\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:44:58.162484Z",
          "iopub.execute_input": "2023-11-23T16:44:58.163004Z",
          "iopub.status.idle": "2023-11-23T16:53:27.831907Z",
          "shell.execute_reply.started": "2023-11-23T16:44:58.162948Z",
          "shell.execute_reply": "2023-11-23T16:53:27.829213Z"
        },
        "trusted": true,
        "id": "QGOfX37YdwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Validation Split (Stratified)"
      ],
      "metadata": {
        "id": "wC23G3gUdwY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(cleansed_train_data_wkCorr.columns)\n",
        "labels=['total_amount']\n",
        "#features = ['VendorID', 'passenger_count', 'trip_distance', 'payment_type', 'extra','tip_amount', 'tolls_amount', 'congestion_surcharge','Airport_fee', 'ride_duration']\n",
        "features = cleansed_train_data_wkCorr.drop(['total_amount','total_amount_dist'], axis=1, inplace = False).columns\n",
        "dist_col = ['total_amount_dist']\n",
        "y = cleansed_train_data_wkCorr[labels]\n",
        "y_dist = cleansed_train_data_wkCorr[dist_col].astype('int')\n",
        "X = cleansed_train_data_wkCorr[features]\n",
        "print(\"X shape: \", X.shape , \" Y Shape: \", y.shape, \" Y_dist Shape: \", y_dist.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:27.833927Z",
          "iopub.execute_input": "2023-11-23T16:53:27.835401Z",
          "iopub.status.idle": "2023-11-23T16:53:27.868831Z",
          "shell.execute_reply.started": "2023-11-23T16:53:27.835344Z",
          "shell.execute_reply": "2023-11-23T16:53:27.86745Z"
        },
        "trusted": true,
        "id": "bzUEnK9KdwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
        "for train_index, test_index in split.split(X, y_dist):\n",
        "    strat_train_set = X.iloc[train_index]\n",
        "    strat_test_set  = X.iloc[test_index]\n",
        "    strat_y_train   = y.iloc[train_index]\n",
        "    strat_y_test    = y.iloc[test_index]\n",
        "\n",
        "print(\"Train set: -> \", strat_train_set.shape, strat_y_train.shape)\n",
        "print(\"Validation set: -> \", strat_test_set.shape, strat_y_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:27.870722Z",
          "iopub.execute_input": "2023-11-23T16:53:27.871087Z",
          "iopub.status.idle": "2023-11-23T16:53:29.277621Z",
          "shell.execute_reply.started": "2023-11-23T16:53:27.871055Z",
          "shell.execute_reply": "2023-11-23T16:53:29.276707Z"
        },
        "trusted": true,
        "id": "xTsTd0_ndwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputer & Scaling"
      ],
      "metadata": {
        "id": "86hGS3UjdwY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cleansed_train_data_new = strat_train_set.copy()\n",
        "print(strat_train_set.columns)\n",
        "print(\"cleansed_train_data shape: \", strat_train_set.shape)\n",
        "strat_train_set.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.278759Z",
          "iopub.execute_input": "2023-11-23T16:53:29.279874Z",
          "iopub.status.idle": "2023-11-23T16:53:29.304133Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.279842Z",
          "shell.execute_reply": "2023-11-23T16:53:29.302915Z"
        },
        "trusted": true,
        "id": "M4EHcOT9dwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Pre-processing**"
      ],
      "metadata": {
        "id": "5F5NLkAcdwY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.305772Z",
          "iopub.execute_input": "2023-11-23T16:53:29.306369Z",
          "iopub.status.idle": "2023-11-23T16:53:29.46838Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.306163Z",
          "shell.execute_reply": "2023-11-23T16:53:29.466949Z"
        },
        "trusted": true,
        "id": "UGzr-ZyidwY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_impute_OE_ecncode_feature = ['passenger_count', 'RatecodeID'] #,'RatecodeID',pickup_hour', ,'passenger_count'\n",
        "cat_impute_OHE_ecncode_feature = ['VendorID','payment_type' ]\n",
        "num_median_scale_feature = ['tip_amount','tolls_amount','congestion_surcharge','Airport_fee','extra'] #,'improvement_surcharge'\n",
        "num_scale_feature = ['trip_distance','ride_duration']\n",
        "labels = ['total_amount']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.469949Z",
          "iopub.execute_input": "2023-11-23T16:53:29.470365Z",
          "iopub.status.idle": "2023-11-23T16:53:29.476662Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.470332Z",
          "shell.execute_reply": "2023-11-23T16:53:29.4756Z"
        },
        "trusted": true,
        "id": "989X0edPdwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impute_median_scale = Pipeline([('impute1', SimpleImputer(strategy = 'median')), ('scale1', StandardScaler())])\n",
        "#num_scale = Pipeline([ ('scale2', StandardScaler())])\n",
        "num_scale = Pipeline([('impute2', SimpleImputer(strategy = 'median')), ('scale2', MinMaxScaler())])\n",
        "\n",
        "impute_cat_encode = Pipeline([('impute3', SimpleImputer(strategy= 'most_frequent')), ('encode3', OrdinalEncoder())])\n",
        "impute_cat_OHencode = Pipeline([('impute4', SimpleImputer(strategy= 'most_frequent')), ('encode4', OneHotEncoder())])\n",
        "\n",
        "#num_const_cat = Pipeline([('impute2', SimpleImputer(strategy = 'mean')), ('encode3', OneHotEncoder())])\n",
        "#impute_mean = Pipeline([('impute2', SimpleImputer(strategy = 'mean')), ('scale4', MinMaxScaler())])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.477915Z",
          "iopub.execute_input": "2023-11-23T16:53:29.4791Z",
          "iopub.status.idle": "2023-11-23T16:53:29.497038Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.479063Z",
          "shell.execute_reply": "2023-11-23T16:53:29.495537Z"
        },
        "trusted": true,
        "id": "6ut8zPPydwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = ColumnTransformer(\n",
        "    [('num_impute_median', impute_median_scale, num_median_scale_feature),\n",
        "     ('num_scale', num_scale, num_scale_feature),\n",
        "     ('impute_cat_OE', impute_cat_encode, cat_impute_OE_ecncode_feature),\n",
        "     ('impute_cat_OHE', impute_cat_OHencode, cat_impute_OHE_ecncode_feature),\n",
        "    ], remainder = 'passthrough', verbose_feature_names_out = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.498613Z",
          "iopub.execute_input": "2023-11-23T16:53:29.499053Z",
          "iopub.status.idle": "2023-11-23T16:53:29.512053Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.499019Z",
          "shell.execute_reply": "2023-11-23T16:53:29.51047Z"
        },
        "trusted": true,
        "id": "NSoSn4_BdwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_preprocessing_logic(cleansed_dataset, is_train_data):\n",
        "    print(\"cleansed_train_data shape (before): \", cleansed_dataset.shape)\n",
        "    transformed_X = preprocess.fit_transform(cleansed_dataset)\n",
        "    prePreocessed_df = pd.DataFrame(transformed_X, columns=preprocess.get_feature_names_out())\n",
        "    print(\"\\n prePreocessed_df shape: \", prePreocessed_df.shape)\n",
        "\n",
        "    print(\"\\n Pre-processed set of Columns: \", prePreocessed_df.columns)\n",
        "    \"\"\"\n",
        "    corr = prePreocessed_df.corr()\n",
        "\n",
        "    if is_train_data:\n",
        "        mlt.rc('figure',figsize=(12,10))\n",
        "        sns.heatmap(corr, annot= True)\n",
        "        print(\"\\n Target Label Corr:\\n\",  corr['remainder__total_amount'])\n",
        "    \"\"\"\n",
        "    return prePreocessed_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.513948Z",
          "iopub.execute_input": "2023-11-23T16:53:29.515249Z",
          "iopub.status.idle": "2023-11-23T16:53:29.526096Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.515201Z",
          "shell.execute_reply": "2023-11-23T16:53:29.524634Z"
        },
        "trusted": true,
        "id": "vtkZLSqidwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* > **Apply Pre-processing to Cleansed Train Data**"
      ],
      "metadata": {
        "id": "sdaEoLObdwY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prePreocessed_Train = apply_preprocessing_logic(strat_train_set, True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.527755Z",
          "iopub.execute_input": "2023-11-23T16:53:29.528778Z",
          "iopub.status.idle": "2023-11-23T16:53:29.973203Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.528741Z",
          "shell.execute_reply": "2023-11-23T16:53:29.971909Z"
        },
        "trusted": true,
        "id": "XwbzKiucdwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleansed_train_data.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.975049Z",
          "iopub.execute_input": "2023-11-23T16:53:29.975419Z",
          "iopub.status.idle": "2023-11-23T16:53:29.983596Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.97539Z",
          "shell.execute_reply": "2023-11-23T16:53:29.981868Z"
        },
        "trusted": true,
        "id": "6O_F7bppdwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* > **Apply Pre-processing to Validation(Train) Data**"
      ],
      "metadata": {
        "id": "3iYxBr8AdwY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation\n",
        "print(\"Validation set: -> \", strat_test_set.shape, strat_y_test.shape)\n",
        "prePreocessed_Validation = apply_preprocessing_logic(strat_test_set, True)\n",
        "display(prePreocessed_Validation.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:29.985135Z",
          "iopub.execute_input": "2023-11-23T16:53:29.985603Z",
          "iopub.status.idle": "2023-11-23T16:53:30.126658Z",
          "shell.execute_reply.started": "2023-11-23T16:53:29.985558Z",
          "shell.execute_reply": "2023-11-23T16:53:30.125424Z"
        },
        "trusted": true,
        "id": "Kw_UeULedwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* > ***Data Preprocessing - Test data***"
      ],
      "metadata": {
        "id": "oo4ZKxEOdwY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head(3)\n",
        "test_data_cleansed = cleansing_module(test_data, False)\n",
        "test_data_cleansed_new = drop_columns_from_df(test_data_cleansed, weak_corr_columns)\n",
        "display(test_data_cleansed_new.head(2))\n",
        "test_data_preProcessed = apply_preprocessing_logic(test_data_cleansed_new, False)\n",
        "#test_data_to_model = drop_columns_from_df(test_data_preProcessed, weak_corr_columns_tr)\n",
        "print(\"\\n----- Test Data to Model ----- \\n\")\n",
        "display(test_data_preProcessed.head(5))\n",
        "display(test_data_preProcessed.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.128211Z",
          "iopub.execute_input": "2023-11-23T16:53:30.128539Z",
          "iopub.status.idle": "2023-11-23T16:53:30.833248Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.128511Z",
          "shell.execute_reply": "2023-11-23T16:53:30.831831Z"
        },
        "trusted": true,
        "id": "qdTOd5jXdwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation"
      ],
      "metadata": {
        "id": "MVLXXBxJdwY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Metrics for testing model's performance\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures, scale, MinMaxScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "results_df = pd.DataFrame()\n",
        "results = []\n",
        "columns = [\"Model\", \"DataSet\" ,\"pred_R2\", \"pred_min\", \"pred_max\", \"exec_start\", \"exec_end\"]\n",
        "print(\"Train set: -> \", strat_train_set.shape, strat_y_train.shape, prePreocessed_Train.shape)\n",
        "\n",
        "def get_method_name():\n",
        "    return inspect.currentframe().f_code.co_name\n",
        "\n",
        "def evaluate_r2(true, predicted):\n",
        "    r2_val = round(r2_score(true, predicted),4)\n",
        "    return r2_val\n",
        "\n",
        "def append_results(model_name, dataset_type, results_df, y_test, pred, exec_start):\n",
        "    results_append = [model_name, dataset_type, evaluate_r2(y_test, pred), np.amin(pred), np.amax(pred), exec_start, datetime.datetime.now()]\n",
        "    results.append(results_append)\n",
        "    results_df = pd.DataFrame(data= results, columns=columns)\n",
        "    return results_df.drop_duplicates()\n",
        "\n",
        "def execute_model(model_n):\n",
        "    \"\"\"\n",
        "    model_n : List, with model_no# to be executed\n",
        "    This executes model in parallel and also maintains return value order as of input list\n",
        "    \"\"\"\n",
        "    model = [models[i] for i in model_n]\n",
        "    print(\"\\n Models to be executed (in parallel, results in order): \\n\")\n",
        "    for m in model:\n",
        "        print(\" - \", m.__name__)\n",
        "\n",
        "    import concurrent.futures\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(m) for m in model]\n",
        "        return_value = [f.result() for f in futures]\n",
        "        print(\"\\n No# of return values: \", len(return_value), \"\\n item-wise return object length:  \", [len(r) for r in return_value])\n",
        "\n",
        "    print(\"\\n ## Execution completed ## \\n Final summary report: \\n\")\n",
        "    #display(results)\n",
        "    print(tabulate(results, headers='firstrow', tablefmt='fancy_grid'))\n",
        "    return return_value"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.836317Z",
          "iopub.execute_input": "2023-11-23T16:53:30.836736Z",
          "iopub.status.idle": "2023-11-23T16:53:30.898914Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.836704Z",
          "shell.execute_reply": "2023-11-23T16:53:30.897567Z"
        },
        "trusted": true,
        "id": "Y3t0mk7EdwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Linear Regression with Polynomial Feature\n"
      ],
      "metadata": {
        "id": "2mJTiT2YdwY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearRegression_with_polynomial():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + LinearRegression_with_polynomial.__qualname__ + \" \"\n",
        "    \"\"\"\n",
        "    LinearRegression_with_polynomial\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n ############## Linear Regression with Polynomial Feature ############## \\n\\n\")\n",
        "\n",
        "    # Fit a polynomial regression model of degree 2\n",
        "    lin_reg = make_pipeline(PolynomialFeatures(degree=3), LinearRegression(fit_intercept=True))\n",
        "    model = lin_reg.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "    # training r2 sccore\n",
        "    y_train_predict=model.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Checking min max of prediction\n",
        "    print(method, \"Predicted -Train: \", np.amin(y_train_predict), np.amax(y_train_predict))\n",
        "    print(method, \"Actuals -Train: \", np.amin(strat_y_train), np.amax(strat_y_train))\n",
        "    append_results(\"Linear Regression\",  \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "    strat_train_set[y_train_predict <0 ] , strat_y_train[y_train_predict <0 ], y_train_predict[y_train_predict <0 ]\n",
        "    #strat_train_set[y_train_predict > 150 ] , strat_y_train[y_train_predict > 150 ], y_train_predict[y_train_predict > 150 ]\n",
        "    y_train_predict_df = pd.DataFrame(y_train_predict, columns= labels)\n",
        "\n",
        "\n",
        "    #Validation Data - Linea with CV-HPT\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model.predict(prePreocessed_Validation)\n",
        "    #Checking min max of prediction\n",
        "    print(method, \"\\nPredicted -Validation: \", np.amin(y_test_predict), np.amax(y_test_predict))\n",
        "    print(method, \"Actuals -Validation: \", np.amin(strat_y_test), np.amax(strat_y_test))\n",
        "    append_results(\"Linear Regression\",  \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    #Prediction\n",
        "    print(method, \"**Prediction on Test data using Linear Regression**\")\n",
        "    test_data_pred_linPoly=model.predict(test_data_preProcessed)\n",
        "    test_data_pred_linPoly.shape\n",
        "\n",
        "    append_results(\"Linear Regression\",  \"Test Data\", results_df, np.zeros(test_data_pred_linPoly.shape), test_data_pred_linPoly,exec_start)\n",
        "\n",
        "    #Delete variables no longer needed\n",
        "    del lin_reg, model\n",
        "\n",
        "    return test_data_pred_linPoly\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.900453Z",
          "iopub.execute_input": "2023-11-23T16:53:30.900813Z",
          "iopub.status.idle": "2023-11-23T16:53:30.917521Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.900782Z",
          "shell.execute_reply": "2023-11-23T16:53:30.915972Z"
        },
        "trusted": true,
        "id": "O-heG0vZdwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Linear Regression with Cross Validation"
      ],
      "metadata": {
        "id": "YjRhJxfOdwY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearRegression_with_CV():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + inspect.currentframe().f_back.f_code.co_name + \" \"\n",
        "    \"\"\"\n",
        "    Linear Regression with Cross Validation\n",
        "    \"\"\"\n",
        "    print(method, \"\\n\\n ############## Linear Regression with Cross Validation ############## \\n\\n\")\n",
        "\n",
        "    # step-1: create a K-FOLD cross-validation scheme\n",
        "    folds = KFold(n_splits = 3, shuffle = True, random_state = random_state)\n",
        "\n",
        "\n",
        "    # step-2: specify range of hyperparameters to tune\n",
        "    \"\"\"\n",
        "    param_grid = [{'n_features_to_select': np.arange(3, 9),\n",
        "                  'estimator__fit_intercept': [True, False]}]\n",
        "    \"\"\"\n",
        "    param_grid = [{'n_features_to_select': np.arange(5, 7),\n",
        "                  'estimator__fit_intercept': [True]}]\n",
        "\n",
        "\n",
        "    # step-3: perform grid search\n",
        "    # 3.1 specify model\n",
        "    lin_reg = LinearRegression()\n",
        "    rfe = RFE(lin_reg)\n",
        "\n",
        "    #lin_reg = make_pipeline(PolynomialFeatures(degree=3), LinearRegression(fit_intercept=True))\n",
        "    #model = lin_reg.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "\n",
        "    # 3.2 call GridSearchCV()\n",
        "    model_cv = GridSearchCV(estimator = rfe,\n",
        "                            param_grid = param_grid,\n",
        "                            scoring= 'r2',\n",
        "                            cv = folds,\n",
        "                            verbose = 1,\n",
        "                            return_train_score=True)\n",
        "\n",
        "    # fit the model\n",
        "    model_cv.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_lin_cv = model_cv.best_estimator_\n",
        "\n",
        "    # training r2 sccore\n",
        "    y_train_predict=model_lin_cv.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    # Display vals\n",
        "    append_results(\"Linear Regression with CV\",  \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "    #Validation Data - Linea with CV-HPT\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_lin_cv.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "    append_results(\"Linear Regression with CV\",  \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"**Prediction on Test data using Linear Regression with CV-HPT**\")\n",
        "    #Prediction\n",
        "    test_data_pred_linCV=model_lin_cv.predict(test_data_preProcessed)\n",
        "    test_data_pred_linCV.shape\n",
        "\n",
        "    append_results(\"Linear Regression with CV\",  \"Test Data\", results_df, np.zeros(test_data_pred_linCV.shape), test_data_pred_linCV,exec_start)\n",
        "\n",
        "    #Delete variables no longer needed\n",
        "    del lin_reg, rfe, model_cv, model_lin_cv\n",
        "\n",
        "    return test_data_pred_linCV"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.918776Z",
          "iopub.execute_input": "2023-11-23T16:53:30.919155Z",
          "iopub.status.idle": "2023-11-23T16:53:30.938299Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.919124Z",
          "shell.execute_reply": "2023-11-23T16:53:30.936532Z"
        },
        "trusted": true,
        "id": "4McyUd0EdwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. KNN with HPT"
      ],
      "metadata": {
        "id": "vVv8NUK6dwY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KNN_with_CV_HPT():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + inspect.currentframe().f_back.f_code.co_name + \" \"\n",
        "    \"\"\"\n",
        "    KNN with HPT\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n ############## KNN with HPT ############## \\n\\n\")\n",
        "\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    \"\"\"\n",
        "    KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "              metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
        "              weights='uniform')\n",
        "    weights{uniform, distance}\n",
        "    algorithm{auto, ball_tree, kd_tree, brute}\n",
        "    n_jobs : int\n",
        "    When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2\n",
        "    \"\"\"\n",
        "\n",
        "    # specify model\n",
        "    knnReg = KNeighborsRegressor()\n",
        "\n",
        "    \"\"\"\n",
        "    # param_grid\n",
        "    param_grid = [{'weights': ['uniform', 'distance'],\n",
        "                  'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "                   'p': [1, 2],\n",
        "                   'n_jobs': [10,20]\n",
        "                  }]\n",
        "    \"\"\"\n",
        "    #Best Hyperparameters: {'weights': 'distance', 'p': 1, 'n_jobs': 20, 'algorithm': 'auto'}\n",
        "\n",
        "    param_grid = [{'weights': ['uniform', 'distance'],\n",
        "                  'algorithm': ['auto'],\n",
        "                   'p': [1, 2],\n",
        "                   'n_jobs': [20, 30]\n",
        "                  }]\n",
        "\n",
        "\n",
        "    # call CV()\n",
        "    model_cv = RandomizedSearchCV(estimator = knnReg,\n",
        "                                  param_distributions = param_grid,\n",
        "                                  scoring= 'r2',\n",
        "                                  cv = 3,\n",
        "                                  verbose = 1,\n",
        "                                  return_train_score=False,\n",
        "                                  random_state=random_state)\n",
        "\n",
        "    # fit the model\n",
        "    model_cv.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_knn_cv = model_cv.best_estimator_\n",
        "\n",
        "    # training r2 sccore\n",
        "    y_train_predict=model_knn_cv.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    # Display vals\n",
        "    append_results(\"KNN with CV\",  \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "    #Validation Data - KNN with CV-HPT\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_knn_cv.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"KNN with CV\",  \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"**Test Data - KNN with CV-HPT***\")\n",
        "    #Prediction\n",
        "    test_data_pred_knnCV=model_knn_cv.predict(test_data_preProcessed)\n",
        "    test_data_pred_knnCV.shape\n",
        "\n",
        "    append_results(\"KNN with CV\",  \"Test Data\", results_df, np.zeros(test_data_pred_knnCV.shape), test_data_pred_knnCV,exec_start)\n",
        "\n",
        "    #Delete variables no longer needed\n",
        "    del model_knn_cv,  model_cv, knnReg\n",
        "\n",
        "    return test_data_pred_knnCV"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.940206Z",
          "iopub.execute_input": "2023-11-23T16:53:30.940619Z",
          "iopub.status.idle": "2023-11-23T16:53:30.959663Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.940588Z",
          "shell.execute_reply": "2023-11-23T16:53:30.958405Z"
        },
        "trusted": true,
        "id": "GQJNcf5SdwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. SVM/SVR"
      ],
      "metadata": {
        "id": "yfithWyvdwY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM_SVR():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + inspect.currentframe().f_back.f_code.co_name + \" \"\n",
        "    \"\"\"\n",
        "    SVM-SVR\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n ############## SVM/SVR ############## \\n\\n\")\n",
        "\n",
        "    from sklearn.svm import SVR\n",
        "    \"\"\"\n",
        "    SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
        "    kernel{linear, poly, rbf, sigmoid, precomputed}\n",
        "    degree: int, default=3 #Degree of the polynomial kernel function (poly).\n",
        "    gamma{scale, auto} or float, default=scale\n",
        "    tol, default=1e-3 #Tolerance for stopping criterion.\n",
        "    C, default=1.0 #Regularization parameter. The strength of the regularization is inversely proportional to C. The penalty is a squared l2 penalty. +ve\n",
        "    epsilon, default=0.1 #Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
        "    \"\"\"\n",
        "    # specify model\n",
        "    svm = SVR()\n",
        "\n",
        "    \"\"\"\n",
        "    # param_grid\n",
        "    param_grid = [{'kernel': ['poly', 'rbf'],\n",
        "                   'degree': [ 2,3], 'tol': [0.001, 0.01],\n",
        "                   'C': [0.1,1],\n",
        "                   'epsilon': [0.1, 0.5]\n",
        "                  }]\n",
        "    #Best Hyperparameters: {'C': 1, 'degree': 3, 'epsilon': 0.5, 'kernel': 'poly', 'tol': 0.001}\n",
        "    \"\"\"\n",
        "    param_grid = [{'kernel': ['poly', 'rbf'],\n",
        "                   'degree': [ 3], 'tol': [0.001],\n",
        "                   'C': [0.1,1],\n",
        "                   'epsilon': [0.5]\n",
        "                  }]\n",
        "\n",
        "    # call CV()\n",
        "    model_cv = GridSearchCV(estimator = svm,\n",
        "                            param_grid = param_grid,\n",
        "                            scoring= 'r2', cv = 2,\n",
        "                            verbose = 1, return_train_score=False)\n",
        "    #model_cv = SVR()\n",
        "\n",
        "    # fit the model\n",
        "    model_cv.fit(prePreocessed_Train.head(1000), strat_y_train.head(1000))\n",
        "\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_svm_cv = model_cv.best_estimator_\n",
        "    #model_svm_cv = model_cv\n",
        "\n",
        "    # training r2 sccore\n",
        "    y_train_predict=model_svm_cv.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    # Display vals\n",
        "    append_results(\"SVR\",  \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "    #Validation Data - SVM/SVR with CV-HPT\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_svm_cv.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"SVR\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"***Test Data - SVR with CV-HPT***\")\n",
        "    #Prediction\n",
        "    test_data_pred_svrCV=model_svm_cv.predict(test_data_preProcessed)\n",
        "    test_data_pred_svrCV.shape\n",
        "\n",
        "    append_results(\"SVR\", \"Test Data\", results_df, np.zeros(test_data_pred_svrCV.shape), test_data_pred_svrCV,exec_start)\n",
        "\n",
        "    #Delete variables no longer needed\n",
        "    del model_svm_cv,  model_cv, svm\n",
        "\n",
        "    return test_data_pred_svrCV"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.961684Z",
          "iopub.execute_input": "2023-11-23T16:53:30.962063Z",
          "iopub.status.idle": "2023-11-23T16:53:30.980521Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.962031Z",
          "shell.execute_reply": "2023-11-23T16:53:30.979086Z"
        },
        "trusted": true,
        "id": "LYUGujV4dwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. CART - DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "ALGxcL5NdwY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CART_DecisionTreeRegressor():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + CART_DecisionTreeRegressor.__qualname__ + \" \"\n",
        "    \"\"\"\n",
        "    CART - classification and regression tasks\n",
        "    \"\"\"\n",
        "\n",
        "    print(method, \"\\n\\n ############## CART - DecisionTreeRegressor ############## \\n\\n\")\n",
        "\n",
        "    from sklearn import tree\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "    #Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand\n",
        "    #min_samples_leaf=5 as an initial value.  min_samples_leaf guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems\n",
        "\n",
        "    \"\"\"\n",
        "    reg= ExtraTreesRegressor()\n",
        "    reg.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "    print(method, \"Important Features: \", reg.feature_importances_)\n",
        "    feat_importances = pd.Series(reg.feature_importances_, index=prePreocessed_Train.columns)\n",
        "    feat_importances.nlargest(8).plot(kind='barh')\n",
        "    plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    # create a regressor object\n",
        "    reg_decision_model=DecisionTreeRegressor(random_state = random_state)\n",
        "\n",
        "    \"\"\"\n",
        "    param_grid = [{'min_samples_leaf': [5, 8, 10, 15],\n",
        "                   'max_depth': [5, 10, 20, 50],\n",
        "                   'max_features': ['sqrt','log2', 'auto'],\n",
        "                   'splitter': ['best','random'],\n",
        "                   'ccp_alpha': [0.1,1,5]\n",
        "                  }]\n",
        "\n",
        "    #Best Hyperparameters: {'ccp_alpha': 0.1, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 10, 'splitter': 'best'}\n",
        "    \"\"\"\n",
        "    param_grid = [{'min_samples_leaf': [8, 10],\n",
        "                   'max_depth': [5, 20],\n",
        "                   'max_features': ['auto'],\n",
        "                   'splitter': ['best'],\n",
        "                   'ccp_alpha': [0.1]\n",
        "                  }]\n",
        "\n",
        "    #call CV()\n",
        "    model_cv = GridSearchCV(estimator = reg_decision_model,\n",
        "                            param_grid = param_grid,\n",
        "                            scoring= 'r2',\n",
        "                            cv = 2,\n",
        "                            verbose = 1, return_train_score=False)\n",
        "\n",
        "    #fit the model\n",
        "    model_cv.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_dcTree = model_cv.best_estimator_\n",
        "\n",
        "    #training r2 sccore\n",
        "    y_train_predict=model_dcTree.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Display vals\n",
        "    append_results(\"Decision Tree Regressor with CV\", \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "    #Validation Data - Decision Tree Regressor\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_dcTree.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"Decision Tree Regressor with CV\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"***Test Data - DecisionTree Regressor CV-HPT***\")\n",
        "    #Prediction\n",
        "    test_data_pred_dcTree=model_dcTree.predict(test_data_preProcessed)\n",
        "    test_data_pred_dcTree.shape\n",
        "\n",
        "    append_results(\"Decision Tree Regressor with CV\", \"Test Data\", results_df, np.zeros(test_data_pred_dcTree.shape), test_data_pred_dcTree,exec_start)\n",
        "\n",
        "    #Delete variables no longer needed\n",
        "    del model_dcTree,  model_cv, reg_decision_model\n",
        "\n",
        "    return test_data_pred_dcTree"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:53:30.982294Z",
          "iopub.execute_input": "2023-11-23T16:53:30.982761Z",
          "iopub.status.idle": "2023-11-23T16:53:31.002537Z",
          "shell.execute_reply.started": "2023-11-23T16:53:30.982723Z",
          "shell.execute_reply": "2023-11-23T16:53:31.001321Z"
        },
        "trusted": true,
        "id": "iz4bPUKZdwY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging & Boosting**\n",
        "\n",
        "- with bootstrap, trees are no longer perfect on the training set\n",
        "- without bootstrap, all trees are perfect on the training set\n",
        "\n",
        "    Bagging: The working principle is to build several base models independently and then to average them for final predictions\n",
        "        (either by voting or by averaging) to form a final prediction.\n",
        "        Most popular bagging estimator is 'Bagging Tress' also knows as 'Random Forest'.\n",
        "        \n",
        "        \n",
        "    Boosting: Boosting models are built sequentially and tries to reduce the bias on final predictions.\n",
        "        Gradient Boosting Machine (GBM)\n",
        "        Extreme Gradient Boosting Machine (XGBM)\n",
        "    \n",
        "    Stacking: The predictions of each individual model are stacked together and used as input to a final estimator to compute the prediction."
      ],
      "metadata": {
        "id": "iUY3goT8dwY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Bagging - RandomForestRegressor\n",
        "\n",
        "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve\n",
        "the predictive accuracy and control over-fitting.\n",
        "The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve\n",
        "the predictive accuracy and control over-fitting.\n",
        "The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree."
      ],
      "metadata": {
        "id": "l8fMWPo8dwY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BAGGING_RandomForestRegressor():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + BAGGING_RandomForestRegressor.__qualname__ + \" \"\n",
        "    \"\"\"\n",
        "    RandomForestRegressor\n",
        "    class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "    min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "    random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n ############## Bagging - RandomForestRegressor ############## \\n\\n\")\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    #create CV\n",
        "    kfold = KFold(n_splits=4, random_state=random_state, shuffle=True)\n",
        "\n",
        "    # create a regressor object\n",
        "    rf_regr=RandomForestRegressor(random_state = random_state, n_jobs=-1)\n",
        "\n",
        "    n_samples = prePreocessed_Train.shape[0]\n",
        "    n_features = prePreocessed_Train.shape[1]\n",
        "\n",
        "    \"\"\"\n",
        "    param_grid = {'n_estimators': [500,100],\n",
        "              'min_samples_split': [2, np.floor(n_samples//1000), 0.3 ],\n",
        "              'min_samples_leaf': [2, 100 ],\n",
        "              'criterion': ['friedman_mse', 'squared_error'],\n",
        "              'max_features': ['sqrt', 1.0],\n",
        "              'bootstrap':[True],\n",
        "              'warm_start': [True],\n",
        "              'ccp_alpha': [0.1, 0.01],\n",
        "              'max_depth': [10, 50],\n",
        "              'n_jobs': [None,30]\n",
        "             }\n",
        "\n",
        "    [INFO] BAGGING_RandomForestRegressor  Best Score: 0.9840257263573695\n",
        "    [INFO] BAGGING_RandomForestRegressor  Best Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.01, 'criterion': 'friedman_mse',\n",
        "            'max_depth': 40, 'max_features': 0.7, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100,\n",
        "            'n_jobs': -1, 'warm_start': True}\n",
        "    [INFO] BAGGING_RandomForestRegressor  0.987094990836529\n",
        "    \"\"\"\n",
        "\n",
        "    param_grid = {'n_estimators': [90],\n",
        "              'min_samples_split': [2],\n",
        "              'min_samples_leaf': [2],\n",
        "              'criterion': ['friedman_mse'],\n",
        "              'max_features': [0.7],\n",
        "              'bootstrap':[True],\n",
        "              'warm_start': [True],\n",
        "              'ccp_alpha': [0.01],\n",
        "              'max_depth': [40],\n",
        "              'n_jobs': [-1]\n",
        "             }\n",
        "\n",
        "    #call CV()\n",
        "    model_cv = GridSearchCV(estimator = rf_regr,\n",
        "                                     param_grid=param_grid,\n",
        "                                     cv=kfold,\n",
        "                                     verbose=10,\n",
        "                                     n_jobs=-1,\n",
        "                                     scoring='r2')\n",
        "\n",
        "    #fit the model\n",
        "    model_cv.fit(prePreocessed_Train, np.ravel(strat_y_train))\n",
        "    #model_cv.fit(prePreocessed_Train, np.ravel(strat_y_train))\n",
        "\n",
        "    #plot_grid_search(model_cv)\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_RandForest = model_cv.best_estimator_\n",
        "\n",
        "    #training r2 sccore\n",
        "    y_train_predict=model_RandForest.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Display vals\n",
        "    append_results(\"Bagging - Random Forest\", \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "    ##Plot Tree\n",
        "    from sklearn import tree\n",
        "    tree.plot_tree(model_RandForest.estimators_[15])\n",
        "\n",
        "    #Validation Data - Bagging - Random Forest\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_RandForest.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"Bagging - Random Forest\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"***Test Data - Bagging: Random Forest***\")\n",
        "    #Prediction\n",
        "    test_data_pred_randForest=model_RandForest.predict(test_data_preProcessed)\n",
        "    test_data_pred_randForest.shape\n",
        "\n",
        "    append_results(\"Bagging - Random Forest\", \"Test Data\", results_df, np.zeros(test_data_pred_randForest.shape), test_data_pred_randForest,exec_start)\n",
        "    \"\"\"\n",
        "    print(\"**** On complete dataset *****\")\n",
        "    prePreocessed_X = apply_preprocessing_logic(X, True)\n",
        "    display(prePreocessed_X.shape)\n",
        "    #fit the model\n",
        "    model_RandForest.fit(prePreocessed_X, np.ravel(y))\n",
        "\n",
        "    print(\"\\n On Validation set-\")\n",
        "    y_test_predict=model_RandForest.predict(prePreocessed_Validation)\n",
        "    append_results(\"Bagging - model_RandForest - X\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    #Prediction\n",
        "    test_data_pred_X_RandForest=model_RandForest.predict(test_data_preProcessed)\n",
        "    display(test_data_pred_X_RandForest.shape)\n",
        "    append_results(\"Bagging - model_RandForest - X\", \"Test Data\", results_df, np.zeros(test_data_pred_X_RandForest.shape)\n",
        "                   , test_data_pred_X_RandForest,exec_start)\n",
        "    return test_data_pred_X_RandForest\n",
        "    \"\"\"\n",
        "\n",
        "    return test_data_pred_randForest"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:29.249881Z",
          "iopub.execute_input": "2023-11-23T16:55:29.250395Z",
          "iopub.status.idle": "2023-11-23T16:55:29.272869Z",
          "shell.execute_reply.started": "2023-11-23T16:55:29.250362Z",
          "shell.execute_reply": "2023-11-23T16:55:29.27142Z"
        },
        "trusted": true,
        "id": "M5irmx3sdwY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Boosting - GradientBoostingRegressor**\n",
        "\n",
        "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced."
      ],
      "metadata": {
        "id": "gLR_2-lhdwY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "SFPcQ0iBdwY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BOOSTING_GradientBoostingRegressor():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + BOOSTING_GradientBoostingRegressor.__qualname__ + \" \"\n",
        "    print(method, \"\\n\\n ############## Boosting - GradientBoostingRegressor ############## \\n\\n\")\n",
        "\n",
        "    from sklearn.ensemble import GradientBoostingRegressor\n",
        "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "    \"\"\"\n",
        "    class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0,\n",
        "    criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
        "    init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1,\n",
        "    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
        "    \"\"\"\n",
        "    # create a regressor object\n",
        "    sgb_regr=GradientBoostingRegressor(random_state = random_state, warm_start=True)\n",
        "\n",
        "    \"\"\"\n",
        "    param_grid = [{'max_depth': [2, 5, 10 ],\n",
        "                  'tol': [1e-7, 1e-5],\n",
        "                  'min_samples_split': [2, 1000, 500 ],\n",
        "                  'min_samples_leaf': [1, 50, 100 ],\n",
        "                  'learning_rate': [0.1, 0.05],\n",
        "                  'loss' : ['squared_error'],\n",
        "                  'n_iter_no_change' : [5],\n",
        "                   'n_estimators' : [100, 200],\n",
        "                   'ccp_alpha' : [0.1, 0.01]\n",
        "                 }]\n",
        "\n",
        "    Best Score: 0.9304809346242513\n",
        "    Best Hyperparameters: {'ccp_alpha': 0.01, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 10,\n",
        "                            'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100,\n",
        "                            'n_iter_no_change': 5, 'tol': 1e-07}\n",
        "    0.9337132705860126\n",
        "    \"\"\"\n",
        "\n",
        "    param_grid = [{'max_depth': [10 ],\n",
        "                  'tol': [1e-7],\n",
        "                  'min_samples_split': [2],\n",
        "                  'min_samples_leaf': [1],\n",
        "                  'learning_rate': [0.1],\n",
        "                  'loss' : ['squared_error'],\n",
        "                  'n_iter_no_change' : [5],\n",
        "                   'n_estimators' : [100],\n",
        "                   'ccp_alpha' : [0.01]\n",
        "                 }]\n",
        "\n",
        "    #call CV()\n",
        "    model_cv = GridSearchCV(estimator = sgb_regr,\n",
        "                                     param_grid=param_grid,\n",
        "                                     n_jobs=-1, cv=3,\n",
        "                                     verbose=1)\n",
        "\n",
        "    #fit the model\n",
        "    model_cv.fit(prePreocessed_Train, np.ravel(strat_y_train))\n",
        "\n",
        "    #plot_grid_search(model_cv)\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_gradBoost = model_cv.best_estimator_\n",
        "\n",
        "    #training r2 sccore\n",
        "    y_train_predict=model_gradBoost.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Display vals\n",
        "    append_results(\"Boosting - GradientBoost\", \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "\n",
        "    #Validation Data - Boosting - GradientBoostHist\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_gradBoost.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"Boosting - GradientBoost\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"***Test Data - Boosting - GradientBoost***\")\n",
        "    #Prediction\n",
        "    test_data_pred_gradBoost=model_gradBoost.predict(test_data_preProcessed)\n",
        "    test_data_pred_gradBoost.shape\n",
        "\n",
        "    append_results(\"Boosting - GradientBoost\", \"Test Data\", results_df, np.zeros(test_data_pred_gradBoost.shape), test_data_pred_gradBoost,exec_start)\n",
        "\n",
        "    del model_gradBoost, model_cv, sgb_regr\n",
        "    return test_data_pred_gradBoost"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:36.046842Z",
          "iopub.execute_input": "2023-11-23T16:55:36.047321Z",
          "iopub.status.idle": "2023-11-23T16:55:36.064159Z",
          "shell.execute_reply.started": "2023-11-23T16:55:36.047281Z",
          "shell.execute_reply": "2023-11-23T16:55:36.062687Z"
        },
        "trusted": true,
        "id": "Rloe66aDdwY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. HistGradientBoostingRegressor"
      ],
      "metadata": {
        "id": "uKLl17dXdwY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BOOSTING_HistGradientBoostingRegressor():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + BOOSTING_HistGradientBoostingRegressor.__qualname__ + \" \"\n",
        "    print(\"\\n\\n ############## Boosting - HistGradientBoostingRegressor ############## \\n\\n\")\n",
        "\n",
        "    from sklearn.ensemble import GradientBoostingRegressor\n",
        "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "    \"\"\"\n",
        "    HistGradientBoostingRegressor estimator is much faster than GradientBoostingRegressor for big datasets (n_samples >= 10 000).\n",
        "    This implementation is inspired by LightGBM.\n",
        "\n",
        "    class sklearn.ensemble.HistGradientBoostingRegressor(loss='squared_error', *, quantile=None, learning_rate=0.1, max_iter=100,\n",
        "    max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_bins=255, categorical_features=None,\n",
        "    monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1,\n",
        "    n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None)[source]\n",
        "    \"\"\"\n",
        "\n",
        "    #create CV\n",
        "    kfold = KFold(n_splits=5, random_state=random_state, shuffle=True)\n",
        "\n",
        "    # create a regressor object\n",
        "    gb_regr=HistGradientBoostingRegressor(random_state = random_state)\n",
        "\n",
        "    \"\"\"\n",
        "    param_grid = [{'max_iter': [None, 300 ],\n",
        "                      'tol': [1e-7],\n",
        "                      'learning_rate': [0.1, 0.01],\n",
        "                      'loss' : ['squared_error'],\n",
        "                      'n_iter_no_change' : [5],\n",
        "                       'max_depth': [None, 100, 10],\n",
        "                       'max_leaf_nodes': [31, 50, 10, None],\n",
        "                       'min_samples_leaf': [10, 20, 50, None],\n",
        "                       'l2_regularization': [None, 0.1, 0.01],\n",
        "                       'warm_start': [True, False],\n",
        "                       'validation_fraction': [0.1]\n",
        "\n",
        "                     }]\n",
        "\n",
        "    [INFO] BOOSTING_HistGradientBoostingRegressor  Best Hyperparameters: {'l2_regularization': 0.01, 'learning_rate': 0.1,\n",
        "            'loss': 'squared_error', 'max_depth': 10, 'max_iter': 300, 'max_leaf_nodes': 300, 'min_samples_leaf': 2,\n",
        "            'n_iter_no_change': 5, 'tol': 1e-07, 'validation_fraction': 0.1, 'warm_start': True}\n",
        "    [INFO] BOOSTING_HistGradientBoostingRegressor  0.9900731561857561\n",
        "    \"\"\"\n",
        "\n",
        "    param_grid = [{'max_iter': [ 300 ],\n",
        "                  'tol': [1e-7, 1e-5],\n",
        "                  'learning_rate': [0.1],\n",
        "                  'loss' : ['squared_error'],\n",
        "                  'n_iter_no_change' : [5],\n",
        "                   'max_depth': [10],\n",
        "                   'max_leaf_nodes': [300],\n",
        "                   'min_samples_leaf': [2],\n",
        "                   'l2_regularization': [0.01],\n",
        "                   'warm_start': [True],\n",
        "                   'validation_fraction': [0.1]\n",
        "\n",
        "                 }]\n",
        "\n",
        "    #call CV()\n",
        "    model_cv = GridSearchCV(estimator = gb_regr,\n",
        "                                     param_grid=param_grid,\n",
        "                                     cv=kfold,\n",
        "                                     verbose=10,\n",
        "                                     n_jobs=-1,\n",
        "                                     scoring='r2')\n",
        "\n",
        "    #fit the model\n",
        "    model_cv.fit(prePreocessed_Train, np.ravel(strat_y_train))\n",
        "\n",
        "    #plot_grid_search(model_cv)\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_gradBoostHist = model_cv.best_estimator_\n",
        "\n",
        "    #training r2 sccore\n",
        "    y_train_predict=model_gradBoostHist.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Display vals\n",
        "    append_results(\"Boosting - GradientBoostHist\", \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "\n",
        "    #Validation Data - Boosting - GradientBoostHist\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_gradBoostHist.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"Boosting - GradientBoostHist\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"***Test Data - Boosting - GradientBoostHist***\")\n",
        "    #Prediction\n",
        "    test_data_pred_gradBoostHist=model_gradBoostHist.predict(test_data_preProcessed)\n",
        "    test_data_pred_gradBoostHist.shape\n",
        "\n",
        "    append_results(\"Boosting - GradientBoostHist\", \"Test Data\", results_df, np.zeros(test_data_pred_gradBoostHist.shape), test_data_pred_gradBoostHist,exec_start)\n",
        "\n",
        "    print(\"**** On complete dataset *****\")\n",
        "    prePreocessed_X = apply_preprocessing_logic(X, True)\n",
        "    display(prePreocessed_X.shape)\n",
        "    #fit the model\n",
        "    model_gradBoostHist.fit(prePreocessed_X, np.ravel(y))\n",
        "\n",
        "    print(\"\\n On Validation set-\")\n",
        "    y_test_predict=model_gradBoostHist.predict(prePreocessed_Validation)\n",
        "    append_results(\"Boosting - model_gradBoostHist - X\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    #Prediction\n",
        "    test_data_pred_X_gradBoostHist=model_gradBoostHist.predict(test_data_preProcessed)\n",
        "    display(test_data_pred_X_gradBoostHist.shape)\n",
        "    append_results(\"Boosting - model_gradBoostHist - X\", \"Test Data\", results_df, np.zeros(test_data_pred_X_gradBoostHist.shape)\n",
        "                   , test_data_pred_X_gradBoostHist,exec_start)\n",
        "\n",
        "    del model_gradBoostHist, model_cv, gb_regr\n",
        "    return test_data_pred_X_gradBoostHist"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:40.042564Z",
          "iopub.execute_input": "2023-11-23T16:55:40.043423Z",
          "iopub.status.idle": "2023-11-23T16:55:40.06762Z",
          "shell.execute_reply.started": "2023-11-23T16:55:40.04338Z",
          "shell.execute_reply": "2023-11-23T16:55:40.066268Z"
        },
        "trusted": true,
        "id": "_vTxtATpdwY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. MLPRegressor\n",
        "\n",
        "Multi-layer Perceptron regressor - is a class of feedforward artificial neural network. This model optimizes the squared error using LBFGS or stochastic gradient descent.\n"
      ],
      "metadata": {
        "id": "oTE0hp06dwY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLPerceptronRegressor():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + MLPerceptronRegressor.__qualname__ + \" \"\n",
        "    print(\"\\n\\n ##############  MLP Regressor ############## \\n\\n\")\n",
        "\n",
        "    from sklearn.neural_network import MLPRegressor\n",
        "    \"\"\"\n",
        "    class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,), activation='relu', *,\n",
        "    solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5,\n",
        "    max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
        "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
        "    \"\"\"\n",
        "\n",
        "    #create CV\n",
        "    kfold = KFold(n_splits=3, random_state=random_state, shuffle=True)\n",
        "\n",
        "    # create a regressor object\n",
        "    mlp_regr=MLPRegressor(random_state = random_state)\n",
        "\n",
        "    \"\"\"\n",
        "     param_grid = [{'solver': ['lbfgs', 'sgd' ],\n",
        "                  'alpha': [0.0001, 0.01],\n",
        "                  'early_stopping' : [True],\n",
        "                  'n_iter_no_change' : [10],\n",
        "                   'activation': ['logistic', 'tanh', 'relu'],\n",
        "                   'learning_rate': ['invscaling', 'adaptive', 'constant'],\n",
        "                   'tol': [0.0001, 1e-5],\n",
        "                   'hidden_layer_sizes': [None, 150, 250],\n",
        "                    'max_iter': [10000, None],\n",
        "                 }]\n",
        "\n",
        "     [INFO] MLPerceptronRegressor  Best Score: 0.5160482155861561\n",
        "     [INFO] MLPerceptronRegressor  Best Hyperparameters: {'activation': 'relu', 'alpha': 0.01, 'early_stopping': True,\n",
        "                 'hidden_layer_sizes': 150, 'learning_rate': 'invscaling', 'max_iter': 10000,\n",
        "                 'n_iter_no_change': 10, 'solver': 'lbfgs', 'tol': 0.0001}\n",
        "     [INFO] MLPerceptronRegressor  0.5523925175829927\n",
        "    \"\"\"\n",
        "\n",
        "    param_grid = [{'solver': ['lbfgs'],\n",
        "                  'alpha': [0.01],\n",
        "                  'early_stopping' : [True],\n",
        "                  'n_iter_no_change' : [10],\n",
        "                   'activation': ['relu'],\n",
        "                   'learning_rate': ['invscaling'],\n",
        "                   'tol': [0.0001],\n",
        "                   'hidden_layer_sizes': [150],\n",
        "                    'max_iter': [10000],\n",
        "                 }]\n",
        "\n",
        "    #call CV()\n",
        "    model_cv = GridSearchCV(estimator = mlp_regr,\n",
        "                                     param_grid=param_grid,\n",
        "                                     cv=kfold,\n",
        "                                     verbose=10,\n",
        "                                     n_jobs=64,\n",
        "                                     scoring='r2')\n",
        "\n",
        "    #fit the model\n",
        "    model_cv.fit(prePreocessed_Train, np.ravel(strat_y_train))\n",
        "\n",
        "    #plot_grid_search(model_cv)\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_mlp = model_cv.best_estimator_\n",
        "\n",
        "    #training r2 sccore\n",
        "    y_train_predict=model_mlp.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Display vals\n",
        "    append_results(\"Multi-Layer Perceptron\", \"Train-training\", results_df, strat_y_train, y_train_predict,datetime.datetime.now())\n",
        "\n",
        "\n",
        "    #Validation Data - Multi-Layer Perceptron\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_mlp.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"Multi-Layer Perceptron\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,datetime.datetime.now())\n",
        "\n",
        "    print(method, \"***Test Data - Multi-Layer Perceptron***\")\n",
        "    #Prediction\n",
        "    test_data_pred_mlp=model_mlp.predict(test_data_preProcessed)\n",
        "    test_data_pred_mlp.shape\n",
        "\n",
        "    append_results(\"Multi-Layer Perceptron\", \"Test Data\", results_df, np.zeros(test_data_pred_mlp.shape), test_data_pred_mlp,datetime.datetime.now())\n",
        "    del model_mlp, model_cv, mlp_regr\n",
        "\n",
        "    return test_data_pred_mlp\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:54.654461Z",
          "iopub.execute_input": "2023-11-23T16:55:54.6549Z",
          "iopub.status.idle": "2023-11-23T16:55:54.67288Z",
          "shell.execute_reply.started": "2023-11-23T16:55:54.654866Z",
          "shell.execute_reply": "2023-11-23T16:55:54.671466Z"
        },
        "trusted": true,
        "id": "aTsaKVWrdwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> - using Complete Train Data for Training**"
      ],
      "metadata": {
        "id": "t0vfNlw3dwY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "prePreocessed_X = apply_preprocessing_logic(X, True)\n",
        "display(prePreocessed_X.shape)\n",
        "#fit the model\n",
        "model_gradBoost.fit(prePreocessed_X, np.ravel(y))\n",
        "\n",
        "print(\"\\n On Validation set-\")\n",
        "y_test_predict=model_gradBoost.predict(prePreocessed_Validation)\n",
        "display(append_results(\"Boosting - model_gradBoost - X\", \"Train-Validation\", results_df, strat_y_test, y_test_predict))\n",
        "\n",
        "#Prediction\n",
        "test_data_pred_X_gradBoostHist=model_gradBoost.predict(test_data_preProcessed)\n",
        "display(test_data_pred_X_gradBoostHist.shape)\n",
        "display(append_results(\"Boosting - model_gradBoost - X\", \"Test Data\", results_df, np.zeros(test_data_pred_X_gradBoostHist.shape), test_data_pred_X_gradBoostHist))\n",
        "\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:54.674975Z",
          "iopub.execute_input": "2023-11-23T16:55:54.675528Z",
          "iopub.status.idle": "2023-11-23T16:55:54.69443Z",
          "shell.execute_reply.started": "2023-11-23T16:55:54.675467Z",
          "shell.execute_reply": "2023-11-23T16:55:54.693164Z"
        },
        "trusted": true,
        "id": "1UM1XbvBdwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Boosting - XGBoost\n",
        "\n",
        "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. Before running XGBoost, we must set three types of parameters: general parameters, booster parameters and task parameters (https://xgboost.readthedocs.io/en/latest/parameter.html):\n",
        "- General parameters relate to which booster we are using to do boosting, commonly tree or linear model\n",
        "- Booster parameters depend on which booster you have chosen\n",
        "- Learning task parameters decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\n"
      ],
      "metadata": {
        "id": "R2irG_20dwY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BOOSTING_XGBoost():\n",
        "    exec_start=datetime.datetime.now()\n",
        "    method = \"[INFO] \" + BOOSTING_XGBoost.__qualname__ + \" \"\n",
        "    print(\"\\n\\n ############## Boosting - BOOSTING_XGBoost ############## \\n\\n\")\n",
        "\n",
        "    import xgboost as xgb\n",
        "    import random\n",
        "    \"\"\"\n",
        "    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
        "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear', random_state=42,\n",
        "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)\n",
        "\n",
        "    XGBoost comes with its own class for storing datasets called DMatrix. It is a highly optimized class for memory and speed.\n",
        "    That's why converting datasets into this format is a requirement for the native XGBoost API.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create regression matrices\n",
        "    dtrain_reg = xgb.DMatrix(prePreocessed_Train, strat_y_train)\n",
        "    dvalidation_reg = xgb.DMatrix(prePreocessed_Validation, strat_y_test)\n",
        "    dtest_reg = xgb.DMatrix(test_data_preProcessed)\n",
        "\n",
        "    # create a regressor object\n",
        "    #xgb_regr=xgb.XGBRegressor(random_state = random_state, evals = [(dtrain_reg, \"train\"), (dvalidation_reg, \"validation\")])\n",
        "    xgb_regr=xgb.XGBRegressor(random_state = random_state)\n",
        "\n",
        "    \"\"\"\n",
        "    param_grid = [{\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\",\n",
        "                   'early_stopping_rounds':[10, 50],\n",
        "                   'n_estimators': [100],\n",
        "                   'booster': ['gbtree'],\n",
        "                   'learning_rate': [0.01],\n",
        "                   'max_depth': [5,10],\n",
        "                   'n_jobs': [-1],\n",
        "                   'colsample_bytree': uniform(0.9, 0.4),\n",
        "                   'num_boost_round': [100]\n",
        "                 }]\n",
        "    \"\"\"\n",
        "    param_grid = [{\"objective\": [\"reg:squarederror\"],\n",
        "                   \"tree_method\": [\"gpu_hist\"],\n",
        "                   'early_stopping_rounds':[10, 20],\n",
        "                   'n_estimators': [100],\n",
        "                   'booster': ['gbtree'],\n",
        "                   'learning_rate': [0.01],\n",
        "                   'max_depth': [5,10],\n",
        "                   'n_jobs': [-1],\n",
        "                   'colsample_bytree': [0.4, 0.9]\n",
        "                 }]\n",
        "\n",
        "    #call CV()\n",
        "    model_cv = RandomizedSearchCV(estimator = xgb_regr,\n",
        "                              param_distributions = param_grid,\n",
        "                              scoring='r2',\n",
        "                              cv = 3,\n",
        "                              verbose = 10,\n",
        "                              n_jobs=128,\n",
        "                              return_train_score=True,\n",
        "                              random_state=random_state,\n",
        "                              n_iter=200)\n",
        "\n",
        "    #fit the model\n",
        "    model_cv.fit(prePreocessed_Train, strat_y_train)\n",
        "\n",
        "    #plot_grid_search(model_cv)\n",
        "    print(method, 'Best Score: %s' % model_cv.best_score_)\n",
        "    print(method, 'Best Hyperparameters: %s' % model_cv.best_params_)\n",
        "\n",
        "    model_gradBoost = model_cv.best_estimator_\n",
        "\n",
        "    #training r2 sccore\n",
        "    y_train_predict=model_gradBoost.predict(prePreocessed_Train)\n",
        "    print(method, r2_score(y_train_predict,strat_y_train))\n",
        "\n",
        "    #Display vals\n",
        "    append_results(\"Boosting - XGBoost\", \"Train-training\", results_df, strat_y_train, y_train_predict,exec_start)\n",
        "\n",
        "\n",
        "    #Validation Data - Boosting - GradientBoostHist\n",
        "    print(method, \"\\n On Validation set-\")\n",
        "    y_test_predict=model_gradBoost.predict(prePreocessed_Validation)\n",
        "    print(method, r2_score(y_test_predict,strat_y_test))\n",
        "\n",
        "    append_results(\"Boosting - XGBoost\", \"Train-Validation\", results_df, strat_y_test, y_test_predict,exec_start)\n",
        "\n",
        "    print(method, \"***Test Data - Boosting - XGBoost***\")\n",
        "    #Prediction\n",
        "    test_data_pred_gradBoost=model_gradBoost.predict(test_data_preProcessed)\n",
        "    test_data_pred_gradBoost.shape\n",
        "\n",
        "    append_results(\"Boosting - XGBoost\", \"Test Data\", results_df, np.zeros(test_data_pred_gradBoost.shape), test_data_pred_gradBoost,exec_start)\n",
        "\n",
        "    del model_gradBoost, model_cv, sgb_regr\n",
        "    return test_data_pred_gradBoost"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:54.696511Z",
          "iopub.execute_input": "2023-11-23T16:55:54.697007Z",
          "iopub.status.idle": "2023-11-23T16:55:54.715976Z",
          "shell.execute_reply.started": "2023-11-23T16:55:54.696971Z",
          "shell.execute_reply": "2023-11-23T16:55:54.714593Z"
        },
        "trusted": true,
        "id": "OE3kCDJ2dwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    0: DummyClassifier,\n",
        "    1: LinearRegression_with_polynomial,\n",
        "    2: LinearRegression_with_CV,\n",
        "    3: KNN_with_CV_HPT,\n",
        "    4: SVM_SVR,\n",
        "    5: CART_DecisionTreeRegressor,\n",
        "    6: BAGGING_RandomForestRegressor,\n",
        "    7: BOOSTING_GradientBoostingRegressor,\n",
        "    8: BOOSTING_HistGradientBoostingRegressor,\n",
        "    9: MLPerceptronRegressor,\n",
        "    10: BOOSTING_XGBoost\n",
        "}"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:54.774007Z",
          "iopub.execute_input": "2023-11-23T16:55:54.774445Z",
          "iopub.status.idle": "2023-11-23T16:55:54.78142Z",
          "shell.execute_reply.started": "2023-11-23T16:55:54.774413Z",
          "shell.execute_reply": "2023-11-23T16:55:54.780085Z"
        },
        "trusted": true,
        "id": "pbM_ZxvQdwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return_value = execute_model([6, 8])\n",
        "\n",
        "#return_value = execute_model([6, 7, 8])"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-11-23T16:55:54.784713Z",
          "iopub.execute_input": "2023-11-23T16:55:54.785248Z"
        },
        "trusted": true,
        "id": "5nC1DkATdwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return_value[0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hj7pzX7HdwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results from different Model**\n",
        "\n",
        "return_value = execute_model([6, 7, 8])\n",
        "\n",
        "0.78/0.79\n",
        "\n",
        " Boosting - GradientBoostHist        Train-training      0.99    -28.86965311570945    145.47853258715259  2023-11-01 15:23:13.034344    2023-11-01 16:01:19.998921   \n",
        "\n",
        " Boosting - GradientBoostHist        Train-Validation  0.9787              -21.9782               145.479  2023-11-01 15:23:13.034344    2023-11-01 16:01:22.336365   \n",
        "\n",
        " Boosting - GradientBoostHist        Test Data         0                   -26.6709               147.01   2023-11-01 15:23:13.034344    2023-11-01 16:01:24.653118   \n",
        "\n",
        " Boosting - model_gradBoostHist - X  Train-Validation  0.982               -22.381                148.916  2023-11-01 15:23:13.034344    2023-11-01 16:07:12.280691   \n",
        "\n",
        " Boosting - model_gradBoostHist - X  Test Data         0                   -35.35                 150.316  2023-11-01 15:23:13.034344    2023-11-01 16:07:12.886320   \n",
        "\n",
        " Bagging - Random Forest             Train-training    0.9873              -22.817                144.713  2023-11-01 15:23:13.034874    2023-11-01 16:07:13.424251   \n",
        "\n",
        " Bagging - Random Forest             Train-Validation  0.9827              -22.7401               143.194  2023-11-01 15:23:13.034874    2023-11-01 16:07:13.561382   \n",
        "\n",
        " Bagging - Random Forest             Test Data         0                   -22.5689               142.783  2023-11-01 15:23:13.034874    2023-11-01 16:07:13.734749   \n",
        "\n",
        " Bagging - model_RandForest - X      Train-Validation  0.9827              -22.7401               143.194  2023-11-01 15:23:13.034874    2023-11-01 16:07:14.334281   \n",
        "\n",
        " Bagging - model_RandForest - X      Test Data         0                   -22.5689               142.783  2023-11-01 15:23:13.034874    2023-11-01 16:07:14.500203   \n",
        "\n",
        "\n",
        "0.86\n",
        "\n",
        " Bagging - Random Forest    Train-training      0.9702    -73.1694272526326    192.71950739541327  2023-11-02 14:28:27.728373    2023-11-02 14:43:24.787740   \n",
        "\n",
        " Bagging - Random Forest    Train-Validation   0.97272             -72.7619               190.336  2023-11-02 14:28:27.728373    2023-11-02 14:43:24.890657   \n",
        "\n",
        " Bagging - Random Forest    Test Data           0                  -60.5048               189.904  2023-11-02 14:28:27.728373    2023-11-02 14:43:25.041927   \n",
        "\n"
      ],
      "metadata": {
        "id": "EY46fLRTdwY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Block"
      ],
      "metadata": {
        "id": "HYCj5QRYdwY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_pred_final = return_value[0]\n",
        "print(\"Final Test result -> shape: \", test_data_pred_final.shape, \" , min: \", np.amin(test_data_pred_final), \" , max: \", np.amax(test_data_pred_final))"
      ],
      "metadata": {
        "trusted": true,
        "id": "sb6Aay5TdwY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data_pred_final.shape)\n",
        "sample_submission_data= pd.DataFrame(test_data_pred_final,columns=[\"total_amount\"])\n",
        "sample_submission_data['ID'] = sample_submission_data.index + 1\n",
        "sample_submission_data = sample_submission_data[[\"ID\", \"total_amount\"]]\n",
        "print('sample_submission_data :', sample_submission_data.shape)\n",
        "sample_submission_data.to_csv('submission.csv', index = False)\n",
        "print(\"THANKS!!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "70fWSEZKdwY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}